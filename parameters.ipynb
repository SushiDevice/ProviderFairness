{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Matrix Factorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Experimento 1 fue realizado con los parámetros por defecto de CORNAC\n",
    "\n",
    "# Experimento 2\n",
    "base_gmf = GMF (\n",
    "    num_factors=16,\n",
    "    reg=0,\n",
    "    num_epochs=20,\n",
    "    batch_size=256,\n",
    "    num_neg=2,\n",
    "    lr=0.001,\n",
    "    learner='rmsprop',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Experimento 3\n",
    "base_gmf = GMF (\n",
    "    num_factors=32,\n",
    "    reg=0.1,\n",
    "    num_epochs=50,\n",
    "    batch_size=64,\n",
    "    num_neg=8,\n",
    "    lr=0.0005,\n",
    "    learner='adagrad',\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAECF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Experimento 1\n",
    "base_vaecf = VAECF(\n",
    "        k=k,\n",
    "        autoencoder_structure=[20],\n",
    "        act_fn=\"tanh\",\n",
    "        likelihood=\"mult\",\n",
    "        n_epochs=100,\n",
    "        batch_size=100,\n",
    "        learning_rate=0.001,\n",
    "        beta=1.0,\n",
    "        use_gpu=True,\n",
    "        verbose=True)\n",
    "\n",
    "# Experimento 2\n",
    "base_vaecf = VAECF(\n",
    "    k=k,\n",
    "    autoencoder_structure=[32, 16],  # Deeper network for more complex patterns\n",
    "    act_fn=\"relu\",                   # ReLU often performs better in deep networks\n",
    "    likelihood=\"mult\",\n",
    "    n_epochs=200,                    # More epochs for better convergence\n",
    "    batch_size=256,                  # Larger batch size for faster training\n",
    "    learning_rate=0.0005,           # Lower learning rate for stability\n",
    "    beta=0.8,                       # Slightly lower beta to reduce KL impact\n",
    "    use_gpu=True,\n",
    "    verbose=True)\n",
    "\n",
    "# Experimento 3\n",
    "base_vaecf = VAECF(\n",
    "    k=k,\n",
    "    autoencoder_structure=[64],      # Wider single layer for more representation capacity\n",
    "    act_fn=\"sigmoid\",               # Sigmoid for smoother gradients\n",
    "    likelihood=\"mult\",\n",
    "    n_epochs=150,                   # Balanced number of epochs\n",
    "    batch_size=64,                  # Smaller batch size for better generalization\n",
    "    learning_rate=0.002,           # Higher learning rate with sigmoid\n",
    "    beta=1.2,                      # Higher beta for stronger regularization\n",
    "    use_gpu=True,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento 1\n",
    "base_neumf = NeuMF (\n",
    "    num_factors= 8,\n",
    "    layers = (64, 32, 16, 8),\n",
    "    act_fn = \"relu\",\n",
    "    reg = 0,\n",
    "    num_epochs= 20,\n",
    "    batch_size = 256,\n",
    "    num_neg = 4,\n",
    "    lr = 0.001,\n",
    "    learner = \"adam\",\n",
    "    backend = \"tensorflow\",\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Experimento 2: Configuración más profunda con regularización\n",
    "base_neumf_deep = NeuMF (\n",
    "    num_factors= 16,                    # Incrementado para capturar más factores latentes\n",
    "    layers = (128, 64, 32, 16),        # Red más profunda para capturar patrones más complejos\n",
    "    act_fn = \"tanh\",                   # Tanh para mejor gradiente en capas profundas\n",
    "    reg = 0.01,                        # Regularización para evitar overfitting\n",
    "    num_epochs= 50,                    # Más épocas para mejor convergencia\n",
    "    batch_size = 128,                  # Batch size más pequeño para mejor generalización\n",
    "    num_neg = 8,                       # Más muestras negativas para mejor discriminación\n",
    "    lr = 0.0005,                       # Learning rate más bajo para estabilidad\n",
    "    learner = \"rmsprop\",               # RMSprop para mejor manejo de gradientes\n",
    "    backend = \"tensorflow\",\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Experimento 3: Configuración con balance fairness-performance\n",
    "base_neumf_fair = NeuMF (\n",
    "    num_factors= 32,                    # Mayor dimensionalidad para representación más rica\n",
    "    layers = (256, 128, 64),           # Capas más anchas pero menos profundas\n",
    "    act_fn = \"elu\",                    # ELU para mejor manejo de sesgos\n",
    "    reg = 0.05,                        # Mayor regularización para reducir sesgos\n",
    "    num_epochs= 30,                    # Balance entre entrenamiento y overfitting\n",
    "    batch_size = 512,                  # Batch size grande para mejor estimación de gradientes\n",
    "    num_neg = 6,                       # Balance en muestras negativas\n",
    "    lr = 0.001,                        # Learning rate estándar\n",
    "    learner = \"adam\",                  # Adam para adaptación automática\n",
    "    backend = \"tensorflow\",\n",
    "    verbose = True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
