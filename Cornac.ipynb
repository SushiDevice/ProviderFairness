{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69ba81caf58e1381",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Imports\n",
    "These experiments were run on Python 3.8. In the requirements.txt are the versions used for these packages.\n",
    "- tqdm: For showing progress in loops.\n",
    "- numpy and pandas: For data manipulation.\n",
    "- cornac: For obtaining the recommendations.\n",
    "- tensorflow: Required by cornac.\n",
    "- torch: Required for the VAECF implementation of Cornac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from logging import Formatter, StreamHandler, getLogger, INFO\n",
    "\n",
    "from tqdm import tqdm\n",
    "from cornac import Experiment\n",
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.metrics import NDCG, Recall, Precision\n",
    "from cornac.hyperopt import Discrete, Continuous\n",
    "from cornac.hyperopt import GridSearch, RandomSearch\n",
    "#from cornac.models import MF, WMF, SVD, VAECF\n",
    "from cornac.models import VAECF, NeuMF, MCF, SVD, WMF, SKMeans, GMF\n",
    "from cornac.exception import ScoreException\n",
    "from numpy import array, nan\n",
    "from pandas import read_csv, DataFrame, Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419102f5de0b9dd3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Logger setup\n",
    "Here we set up the logger for showing some info when executing this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "2daab4dcf47f0e24",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger = getLogger(__name__)\n",
    "logger.setLevel(INFO)\n",
    "\n",
    "ch = StreamHandler()\n",
    "ch.setLevel(INFO)\n",
    "ch.setFormatter(\n",
    "    Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    ")\n",
    "\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c488da52c0b011",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Configuration variables\n",
    "We define some variables used on the rest of the experiment.\n",
    "\n",
    "### General config\n",
    "Getting the date now and the name of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "d635b3d3d5b23e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "now = f'{datetime.now():%Y%m%d%H%M}'\n",
    "experiment_name = 'AMBAR'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c6a66141bf440",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### File and dir config\n",
    "Getting the working directory with pathlib, and obtaining the csv to be used in cornac, and defining a results directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "2b5c46e25bce6b97",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "work_dir = Path('.').resolve()\n",
    "data_file = work_dir / 'data' / experiment_name / 'ratings_info.csv'\n",
    "results_dir = work_dir / 'results' / experiment_name / now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51611283d1977550",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here we make sure the results directory exists by creating it if it doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "a635f7c30ba080d2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not results_dir.exists():\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaad9a156a91656b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Also, we make sure the data file exists and is a file. Here we could also make sure that the file is an actual csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "d97ea640067e44fd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not data_file.exists() and data_file.is_file():\n",
    "    print(\"Bad data file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5bb98578bc357",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Dataframe config\n",
    "We define the names of the headers of each column to be identified by pandas. Also, we define the data type of the values in each cell of the user, item and rating. If the data has multiple data types, the val_dtype can be a list of type string compatible with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "79e39b6dbc03ed72",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_names = {\n",
    "    'user': 'user_id',\n",
    "    'item': 'track_id',\n",
    "    'rating': 'rating'\n",
    "}\n",
    "val_dtype = 'int'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f3e896f286446",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Cornac config\n",
    "Here we set up the k value, the test set size and the validation set size. Also we decide if we want to exclude unknown values or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "f569e7c7b4df6096",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 1000\n",
    "test_size = 0.2\n",
    "val_size = 0.1\n",
    "exclude_unknown = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab5851fd6f35830",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Function setup\n",
    "We set up various utility functions to be used later. Mostly for exporting data and getting it in a format compatible with cornac.\n",
    "\n",
    "set_data_to_tuple_list takes a dict of {user: [item_list, rating_list]}, process it and returns a tuple list of format [(user, item, rating)...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "3f809e7ca6f8cedc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_data_to_tuple_list(d: dict) -> list:\n",
    "    result = []\n",
    "    for user in d:\n",
    "        transpose = array(d[user]).T\n",
    "        for t in transpose:\n",
    "            result.append((user,) + tuple(t))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5bd6bbdad0dba2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "list_to_dict converts a list into a dict using dict comprehension and enumerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "692ef4f13a3a5c51",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def list_to_dict(l: list) -> dict:\n",
    "    return {i: v for i, v in enumerate(l)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b75cb4025f5f7fa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "get_set_dataframe process the raw data ({user: [item_list, rating_list]}), with the item ids and user ids, and converts it into a pandas DataFrame to be exported later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "d0f5aba5f16befe8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transforma de formato ({user:[item_list, rating_list]})\n",
    "# DataFrame final:\n",
    "#    user_id  item_id  rating  item_idx  user_idx\n",
    "# 0        0        0       5         0         0\n",
    "# 1        0        1       3         1         0\n",
    "# 2        1        1       4         1         1\n",
    "# 3        1        2       2         2         1\n",
    "def get_set_dataframe(set_data: dict, i_ids: list, u_ids: list) -> DataFrame:\n",
    "    data_list = set_data_to_tuple_list(set_data)\n",
    "    i_map = list_to_dict(i_ids)\n",
    "    u_map = list_to_dict(u_ids)\n",
    "\n",
    "    set_df = DataFrame(data_list,\n",
    "                       columns=list(col_names.values()),\n",
    "                       dtype=val_dtype)\n",
    "    set_df['item_idx'] = set_df[col_names['item']]\n",
    "    set_df['item'] = set_df[col_names['item']].replace(to_replace=i_map)\n",
    "    set_df['user_idx'] = set_df[col_names['user']]\n",
    "    set_df['user'] = set_df[col_names['user']].replace(to_replace=u_map)\n",
    "    return set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "c2f3457a3965bf91",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 00:11:28,389 - INFO - Experiment start...\n",
      "2025-01-06 00:11:28,389 - INFO - Experiment start...\n",
      "2025-01-06 00:11:28,389 - INFO - Experiment start...\n",
      "2025-01-06 00:11:28,389 - INFO - Experiment start...\n",
      "2025-01-06 00:11:28,389 - INFO - Experiment start...\n",
      "2025-01-06 00:11:28,389 - INFO - Experiment start...\n",
      "2025-01-06 00:11:28,389 - INFO - Experiment start...\n",
      "2025-01-06 00:11:28,389 - INFO - Experiment start...\n",
      "2025-01-06 00:11:28,389 - INFO - Experiment start...\n",
      "2025-01-06 00:11:28,389 - INFO - Experiment start...\n",
      "2025-01-06 00:11:28,389 - INFO - Experiment start...\n",
      "2025-01-06 00:11:28,389 - INFO - Experiment start...\n",
      "2025-01-06 00:11:28,389 - INFO - Experiment start...\n",
      "2025-01-06 00:11:28,396 - INFO - AMBAR\n",
      "2025-01-06 00:11:28,396 - INFO - AMBAR\n",
      "2025-01-06 00:11:28,396 - INFO - AMBAR\n",
      "2025-01-06 00:11:28,396 - INFO - AMBAR\n",
      "2025-01-06 00:11:28,396 - INFO - AMBAR\n",
      "2025-01-06 00:11:28,396 - INFO - AMBAR\n",
      "2025-01-06 00:11:28,396 - INFO - AMBAR\n",
      "2025-01-06 00:11:28,396 - INFO - AMBAR\n",
      "2025-01-06 00:11:28,396 - INFO - AMBAR\n",
      "2025-01-06 00:11:28,396 - INFO - AMBAR\n",
      "2025-01-06 00:11:28,396 - INFO - AMBAR\n",
      "2025-01-06 00:11:28,396 - INFO - AMBAR\n",
      "2025-01-06 00:11:28,396 - INFO - AMBAR\n",
      "2025-01-06 00:11:28,400 - INFO - k=1000\n",
      "2025-01-06 00:11:28,400 - INFO - k=1000\n",
      "2025-01-06 00:11:28,400 - INFO - k=1000\n",
      "2025-01-06 00:11:28,400 - INFO - k=1000\n",
      "2025-01-06 00:11:28,400 - INFO - k=1000\n",
      "2025-01-06 00:11:28,400 - INFO - k=1000\n",
      "2025-01-06 00:11:28,400 - INFO - k=1000\n",
      "2025-01-06 00:11:28,400 - INFO - k=1000\n",
      "2025-01-06 00:11:28,400 - INFO - k=1000\n",
      "2025-01-06 00:11:28,400 - INFO - k=1000\n",
      "2025-01-06 00:11:28,400 - INFO - k=1000\n",
      "2025-01-06 00:11:28,400 - INFO - k=1000\n",
      "2025-01-06 00:11:28,400 - INFO - k=1000\n",
      "2025-01-06 00:11:28,407 - INFO - work_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9')\n",
      "2025-01-06 00:11:28,407 - INFO - work_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9')\n",
      "2025-01-06 00:11:28,407 - INFO - work_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9')\n",
      "2025-01-06 00:11:28,407 - INFO - work_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9')\n",
      "2025-01-06 00:11:28,407 - INFO - work_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9')\n",
      "2025-01-06 00:11:28,407 - INFO - work_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9')\n",
      "2025-01-06 00:11:28,407 - INFO - work_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9')\n",
      "2025-01-06 00:11:28,407 - INFO - work_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9')\n",
      "2025-01-06 00:11:28,407 - INFO - work_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9')\n",
      "2025-01-06 00:11:28,407 - INFO - work_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9')\n",
      "2025-01-06 00:11:28,407 - INFO - work_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9')\n",
      "2025-01-06 00:11:28,407 - INFO - work_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9')\n",
      "2025-01-06 00:11:28,407 - INFO - work_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9')\n",
      "2025-01-06 00:11:28,415 - INFO - data_file=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/data/AMBAR/ratings_info.csv')\n",
      "2025-01-06 00:11:28,415 - INFO - data_file=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/data/AMBAR/ratings_info.csv')\n",
      "2025-01-06 00:11:28,415 - INFO - data_file=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/data/AMBAR/ratings_info.csv')\n",
      "2025-01-06 00:11:28,415 - INFO - data_file=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/data/AMBAR/ratings_info.csv')\n",
      "2025-01-06 00:11:28,415 - INFO - data_file=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/data/AMBAR/ratings_info.csv')\n",
      "2025-01-06 00:11:28,415 - INFO - data_file=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/data/AMBAR/ratings_info.csv')\n",
      "2025-01-06 00:11:28,415 - INFO - data_file=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/data/AMBAR/ratings_info.csv')\n",
      "2025-01-06 00:11:28,415 - INFO - data_file=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/data/AMBAR/ratings_info.csv')\n",
      "2025-01-06 00:11:28,415 - INFO - data_file=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/data/AMBAR/ratings_info.csv')\n",
      "2025-01-06 00:11:28,415 - INFO - data_file=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/data/AMBAR/ratings_info.csv')\n",
      "2025-01-06 00:11:28,415 - INFO - data_file=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/data/AMBAR/ratings_info.csv')\n",
      "2025-01-06 00:11:28,415 - INFO - data_file=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/data/AMBAR/ratings_info.csv')\n",
      "2025-01-06 00:11:28,415 - INFO - data_file=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/data/AMBAR/ratings_info.csv')\n",
      "2025-01-06 00:11:28,422 - INFO - results_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/results/AMBAR/202501060011')\n",
      "2025-01-06 00:11:28,422 - INFO - results_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/results/AMBAR/202501060011')\n",
      "2025-01-06 00:11:28,422 - INFO - results_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/results/AMBAR/202501060011')\n",
      "2025-01-06 00:11:28,422 - INFO - results_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/results/AMBAR/202501060011')\n",
      "2025-01-06 00:11:28,422 - INFO - results_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/results/AMBAR/202501060011')\n",
      "2025-01-06 00:11:28,422 - INFO - results_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/results/AMBAR/202501060011')\n",
      "2025-01-06 00:11:28,422 - INFO - results_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/results/AMBAR/202501060011')\n",
      "2025-01-06 00:11:28,422 - INFO - results_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/results/AMBAR/202501060011')\n",
      "2025-01-06 00:11:28,422 - INFO - results_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/results/AMBAR/202501060011')\n",
      "2025-01-06 00:11:28,422 - INFO - results_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/results/AMBAR/202501060011')\n",
      "2025-01-06 00:11:28,422 - INFO - results_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/results/AMBAR/202501060011')\n",
      "2025-01-06 00:11:28,422 - INFO - results_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/results/AMBAR/202501060011')\n",
      "2025-01-06 00:11:28,422 - INFO - results_dir=WindowsPath('M:/Framework/C-Fairness-RecSys/Provider/AMBAR-35d26542c99acb47dbf0e8e08d6d97b19b27c6b9/results/AMBAR/202501060011')\n"
     ]
    }
   ],
   "source": [
    "logger.info('Experiment start...')\n",
    "logger.info(f'{experiment_name}')\n",
    "logger.info(f'{k=}')\n",
    "logger.info(f'{work_dir=}')\n",
    "logger.info(f'{data_file=}')\n",
    "logger.info(f'{results_dir=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7017668614f3b926",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here we create the dataset out of the data file, the expected data is only with user, item and rating in that order. The name of the columns is defined in the set-up part, same with the data types.\n",
    "\n",
    "For testing purposes before actually executing the full experiment, we left a filter that takes a sample of 50 users, and gets only the data of those 50 users. Please use it only to make sure that the script executes correctly from start to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "d16d3a7ebfcee39b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 00:11:28,439 - INFO - Loading data into triplets...\n",
      "2025-01-06 00:11:28,439 - INFO - Loading data into triplets...\n",
      "2025-01-06 00:11:28,439 - INFO - Loading data into triplets...\n",
      "2025-01-06 00:11:28,439 - INFO - Loading data into triplets...\n",
      "2025-01-06 00:11:28,439 - INFO - Loading data into triplets...\n",
      "2025-01-06 00:11:28,439 - INFO - Loading data into triplets...\n",
      "2025-01-06 00:11:28,439 - INFO - Loading data into triplets...\n",
      "2025-01-06 00:11:28,439 - INFO - Loading data into triplets...\n",
      "2025-01-06 00:11:28,439 - INFO - Loading data into triplets...\n",
      "2025-01-06 00:11:28,439 - INFO - Loading data into triplets...\n",
      "2025-01-06 00:11:28,439 - INFO - Loading data into triplets...\n",
      "2025-01-06 00:11:28,439 - INFO - Loading data into triplets...\n",
      "2025-01-06 00:11:28,439 - INFO - Loading data into triplets...\n"
     ]
    }
   ],
   "source": [
    "# user, item, rating\n",
    "keys = ['0', '1', '2']\n",
    "\n",
    "# Crea un diccionario que mapea cada clave a su tipo de dato\n",
    "if isinstance(val_dtype, str):\n",
    "    d_type = {key: val_dtype for key in keys}\n",
    "elif isinstance(val_dtype, list):\n",
    "    d_type = dict(zip(keys, val_dtype))\n",
    "else:\n",
    "    logger.error('Wrong type setup. Must be a type string or a list of type string.')\n",
    "    exit()\n",
    "\n",
    "logger.info('Loading data into triplets...')\n",
    "df = read_csv(\n",
    "    data_file,\n",
    "    header=0,\n",
    "    names=['0', '1', '2']\n",
    ")[['0', '1', '2']].astype(d_type)\n",
    "\n",
    "# FOR TESTING ONLY\n",
    "# Selecciona aleatoriamente 50 usuarios unicos del dataframe\n",
    "#user_filter = Series(df['0'].unique()).sample(50).to_list()\n",
    "# Incluye solo filas donde estan los usuarios filtrados\n",
    "#df = df[df['0'].isin(user_filter)]\n",
    "\n",
    "data = list(df.to_records(index=False, column_dtypes=d_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300bbb3260fd4305",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here we create the Ratio Split that will be used by cornac. It splits the data into 3 sets randomly. 1 for test, 1 for train and 1 for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "c9e6e383e56136f1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 00:11:28,591 - INFO - Creating ratio split...\n",
      "2025-01-06 00:11:28,591 - INFO - Creating ratio split...\n",
      "2025-01-06 00:11:28,591 - INFO - Creating ratio split...\n",
      "2025-01-06 00:11:28,591 - INFO - Creating ratio split...\n",
      "2025-01-06 00:11:28,591 - INFO - Creating ratio split...\n",
      "2025-01-06 00:11:28,591 - INFO - Creating ratio split...\n",
      "2025-01-06 00:11:28,591 - INFO - Creating ratio split...\n",
      "2025-01-06 00:11:28,591 - INFO - Creating ratio split...\n",
      "2025-01-06 00:11:28,591 - INFO - Creating ratio split...\n",
      "2025-01-06 00:11:28,591 - INFO - Creating ratio split...\n",
      "2025-01-06 00:11:28,591 - INFO - Creating ratio split...\n",
      "2025-01-06 00:11:28,591 - INFO - Creating ratio split...\n",
      "2025-01-06 00:11:28,591 - INFO - Creating ratio split...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating_threshold = 1.0\n",
      "exclude_unknowns = True\n",
      "---\n",
      "Training data:\n",
      "Number of users = 1288\n",
      "Number of items = 28940\n",
      "Number of ratings = 74517\n",
      "Max rating = 5.0\n",
      "Min rating = 1.0\n",
      "Global mean = 1.6\n",
      "---\n",
      "Test data:\n",
      "Number of users = 1288\n",
      "Number of items = 28940\n",
      "Number of ratings = 15789\n",
      "Number of unknown users = 0\n",
      "Number of unknown items = 0\n",
      "---\n",
      "Validation data:\n",
      "Number of users = 1288\n",
      "Number of items = 28940\n",
      "Number of ratings = 7875\n",
      "---\n",
      "Total users = 1288\n",
      "Total items = 28940\n"
     ]
    }
   ],
   "source": [
    "logger.info('Creating ratio split...')\n",
    "ratio_split = RatioSplit(\n",
    "    data=data,\n",
    "    test_size=test_size,\n",
    "    val_size=val_size,\n",
    "    exclude_unknowns=exclude_unknown,\n",
    "    verbose=True,\n",
    "    seed=123 # Añadida para poder reproducir resultados\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab41cdeeccb9adf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We define the metris here. In this experiment, we set up NDCG, Recall and Precision, using the k defined in the set-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "34a2aa4217a63dfc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    NDCG(k),\n",
    "    Recall(k),\n",
    "    Precision(k)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e4a74ce9da083",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Also, we define the models with some previously obtained parameters. We could also define the hyperparameter calculation in this part, in this case, is important to leave a models variable with said configuration, so cornac can pick up the array and execute the calculation and exporting of the recommendations.\n",
    "\n",
    "Because this script is assuming an array with models with parameters already predefined, in case of needing the best parameters obtained by cornac, the exporting of this must be done after running the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaeb6cc",
   "metadata": {},
   "source": [
    "## Base models to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "8bfd5c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_vaecf = VAECF(\n",
    "    name='vaecf_default',\n",
    "    k=k,\n",
    "    autoencoder_structure=[20],\n",
    "    act_fn=\"tanh\",\n",
    "    likelihood=\"mult\",\n",
    "    n_epochs=100,\n",
    "    batch_size=100,\n",
    "    learning_rate=0.001,\n",
    "    beta=1.0,\n",
    "    use_gpu=True,\n",
    "        verbose=True)\n",
    "\n",
    "base_vaecf_a32 = VAECF(\n",
    "    name='vaecf_a32',\n",
    "    k=k,\n",
    "    autoencoder_structure=[32, 16],  # Deeper network for more complex patterns\n",
    "    act_fn=\"relu\",                   # ReLU often performs better in deep networks\n",
    "    likelihood=\"mult\",\n",
    "    n_epochs=200,                    # More epochs for better convergence\n",
    "    batch_size=256,                  # Larger batch size for faster training\n",
    "    learning_rate=0.0005,           # Lower learning rate for stability\n",
    "    beta=0.8,                       # Slightly lower beta to reduce KL impact\n",
    "    use_gpu=True,\n",
    "    verbose=True)\n",
    "\n",
    "base_vaecf_a64 = VAECF(\n",
    "    name='vaecf_a64',\n",
    "    k=k,\n",
    "    autoencoder_structure=[64],      # Wider single layer for more representation capacity\n",
    "    act_fn=\"sigmoid\",               # Sigmoid for smoother gradients\n",
    "    likelihood=\"mult\",\n",
    "    n_epochs=150,                   # Balanced number of epochs\n",
    "    batch_size=64,                  # Smaller batch size for better generalization\n",
    "    learning_rate=0.002,           # Higher learning rate with sigmoid\n",
    "    beta=1.2,                      # Higher beta for stronger regularization\n",
    "    use_gpu=True,\n",
    "    verbose=True)\n",
    "\n",
    "# No usar, aun necesita testeo\n",
    "base_mcf = MCF (\n",
    "        k=k,\n",
    "        max_iter= 100,\n",
    "        learning_rate= 0.001,\n",
    "        gamma = 0.9,\n",
    "        lamda= 0.001,  \n",
    "        verbose=True,\n",
    "        #init_params= any\n",
    "        #falta informacion del contexto\n",
    ")\n",
    "\n",
    "base_neumf = NeuMF (\n",
    "    name= 'neumf_default',\n",
    "    num_factors= 8,\n",
    "    layers = (64, 32, 16, 8),\n",
    "    act_fn = \"relu\",\n",
    "    reg = 0,\n",
    "    num_epochs= 20,\n",
    "    batch_size = 256,\n",
    "    num_neg = 4,\n",
    "    lr = 0.001,\n",
    "    learner = \"adam\",\n",
    "    backend = \"tensorflow\",\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "base_neumf_deep = NeuMF (\n",
    "    name='neumf_deep',\n",
    "    num_factors= 16,                    # Incrementado para capturar más factores latentes\n",
    "    layers = (128, 64, 32, 16),        # Red más profunda para capturar patrones más complejos\n",
    "    act_fn = \"tanh\",                   # Tanh para mejor gradiente en capas profundas\n",
    "    reg = 0.01,                        # Regularización para evitar overfitting\n",
    "    num_epochs= 50,                    # Más épocas para mejor convergencia\n",
    "    batch_size = 128,                  # Batch size más pequeño para mejor generalización\n",
    "    num_neg = 8,                       # Más muestras negativas para mejor discriminación\n",
    "    lr = 0.0005,                       # Learning rate más bajo para estabilidad\n",
    "    learner = \"rmsprop\",               # RMSprop para mejor manejo de gradientes\n",
    "    backend = \"tensorflow\",\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Experimento 3: Configuración con balance fairness-performance\n",
    "base_neumf_fair = NeuMF (\n",
    "    name='neumf_fair',\n",
    "    num_factors= 32,                    # Mayor dimensionalidad para representación más rica\n",
    "    layers = (256, 128, 64),           # Capas más anchas pero menos profundas\n",
    "    act_fn = \"elu\",                    # ELU para mejor manejo de sesgos\n",
    "    reg = 0.05,                        # Mayor regularización para reducir sesgos\n",
    "    num_epochs= 30,                    # Balance entre entrenamiento y overfitting\n",
    "    batch_size = 512,                  # Batch size grande para mejor estimación de gradientes\n",
    "    num_neg = 6,                       # Balance en muestras negativas\n",
    "    lr = 0.001,                        # Learning rate estándar\n",
    "    learner = \"adam\",                  # Adam para adaptación automática\n",
    "    backend = \"tensorflow\",\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "base_svd =  SVD(\n",
    "        max_iter=5,\n",
    "        k=k,\n",
    "        early_stop=True,\n",
    "        verbose=True,\n",
    "        lambda_reg=0.0001,\n",
    "        learning_rate=0.0001\n",
    "    )\n",
    "\n",
    "base_skm = SKMeans (\n",
    "    k = 5,\n",
    "    max_iter= 100,\n",
    "    tol=1e-6,\n",
    "    verbose=True\n",
    "    \n",
    ")\n",
    "\n",
    "base_gmf = GMF (\n",
    "    num_factors=32,\n",
    "    reg=0.1,\n",
    "    num_epochs=50,\n",
    "    batch_size=64,\n",
    "    num_neg=8,\n",
    "    lr=0.0005,\n",
    "    learner='adagrad',\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "03840305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos que le pasaremos a CORNAC para ejecutar los experimentos \n",
    "models = [\n",
    "   base_neumf,\n",
    "   base_neumf_deep,\n",
    "   base_neumf_fair\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "e9d059a8baf22a2d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 00:11:29,244 - INFO - total_users=1288\n",
      "2025-01-06 00:11:29,244 - INFO - total_users=1288\n",
      "2025-01-06 00:11:29,244 - INFO - total_users=1288\n",
      "2025-01-06 00:11:29,244 - INFO - total_users=1288\n",
      "2025-01-06 00:11:29,244 - INFO - total_users=1288\n",
      "2025-01-06 00:11:29,244 - INFO - total_users=1288\n",
      "2025-01-06 00:11:29,244 - INFO - total_users=1288\n",
      "2025-01-06 00:11:29,244 - INFO - total_users=1288\n",
      "2025-01-06 00:11:29,244 - INFO - total_users=1288\n",
      "2025-01-06 00:11:29,244 - INFO - total_users=1288\n",
      "2025-01-06 00:11:29,244 - INFO - total_users=1288\n",
      "2025-01-06 00:11:29,244 - INFO - total_users=1288\n",
      "2025-01-06 00:11:29,244 - INFO - total_users=1288\n",
      "2025-01-06 00:11:29,254 - INFO - total_items=28940\n",
      "2025-01-06 00:11:29,254 - INFO - total_items=28940\n",
      "2025-01-06 00:11:29,254 - INFO - total_items=28940\n",
      "2025-01-06 00:11:29,254 - INFO - total_items=28940\n",
      "2025-01-06 00:11:29,254 - INFO - total_items=28940\n",
      "2025-01-06 00:11:29,254 - INFO - total_items=28940\n",
      "2025-01-06 00:11:29,254 - INFO - total_items=28940\n",
      "2025-01-06 00:11:29,254 - INFO - total_items=28940\n",
      "2025-01-06 00:11:29,254 - INFO - total_items=28940\n",
      "2025-01-06 00:11:29,254 - INFO - total_items=28940\n",
      "2025-01-06 00:11:29,254 - INFO - total_items=28940\n",
      "2025-01-06 00:11:29,254 - INFO - total_items=28940\n",
      "2025-01-06 00:11:29,254 - INFO - total_items=28940\n"
     ]
    }
   ],
   "source": [
    "# Obtener el total de usuarios del split de entrenamiento\n",
    "total_users = ratio_split.train_set.num_users\n",
    "# Obtener el total de items del split de entrenamiento\n",
    "total_items = ratio_split.train_set.num_items\n",
    "logger.info(f'{total_users=}')\n",
    "logger.info(f'{total_items=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d75765eba73247",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After setting up the metrics and models, we export the test, train and validation data into the results directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "cd3120168e041a6a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 00:11:29,275 - INFO - Exporting test data...\n",
      "2025-01-06 00:11:29,275 - INFO - Exporting test data...\n",
      "2025-01-06 00:11:29,275 - INFO - Exporting test data...\n",
      "2025-01-06 00:11:29,275 - INFO - Exporting test data...\n",
      "2025-01-06 00:11:29,275 - INFO - Exporting test data...\n",
      "2025-01-06 00:11:29,275 - INFO - Exporting test data...\n",
      "2025-01-06 00:11:29,275 - INFO - Exporting test data...\n",
      "2025-01-06 00:11:29,275 - INFO - Exporting test data...\n",
      "2025-01-06 00:11:29,275 - INFO - Exporting test data...\n",
      "2025-01-06 00:11:29,275 - INFO - Exporting test data...\n",
      "2025-01-06 00:11:29,275 - INFO - Exporting test data...\n",
      "2025-01-06 00:11:29,275 - INFO - Exporting test data...\n",
      "2025-01-06 00:11:29,275 - INFO - Exporting test data...\n"
     ]
    }
   ],
   "source": [
    "logger.info('Exporting test data...')\n",
    "get_set_dataframe(\n",
    "    dict(ratio_split.test_set.user_data),\n",
    "    list(ratio_split.test_set.item_ids),\n",
    "    list(ratio_split.test_set.user_ids),\n",
    ").to_csv(results_dir / 'test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "3cd0d1fd06dba550",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 00:11:31,240 - INFO - Exporting train data...\n",
      "2025-01-06 00:11:31,240 - INFO - Exporting train data...\n",
      "2025-01-06 00:11:31,240 - INFO - Exporting train data...\n",
      "2025-01-06 00:11:31,240 - INFO - Exporting train data...\n",
      "2025-01-06 00:11:31,240 - INFO - Exporting train data...\n",
      "2025-01-06 00:11:31,240 - INFO - Exporting train data...\n",
      "2025-01-06 00:11:31,240 - INFO - Exporting train data...\n",
      "2025-01-06 00:11:31,240 - INFO - Exporting train data...\n",
      "2025-01-06 00:11:31,240 - INFO - Exporting train data...\n",
      "2025-01-06 00:11:31,240 - INFO - Exporting train data...\n",
      "2025-01-06 00:11:31,240 - INFO - Exporting train data...\n",
      "2025-01-06 00:11:31,240 - INFO - Exporting train data...\n",
      "2025-01-06 00:11:31,240 - INFO - Exporting train data...\n"
     ]
    }
   ],
   "source": [
    "logger.info('Exporting train data...')\n",
    "get_set_dataframe(\n",
    "    dict(ratio_split.train_set.user_data),\n",
    "    list(ratio_split.train_set.item_ids),\n",
    "    list(ratio_split.train_set.user_ids),\n",
    ").to_csv(results_dir / 'train_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "f6986140f5a050d1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 00:11:35,454 - INFO - Exporting validation data...\n",
      "2025-01-06 00:11:35,454 - INFO - Exporting validation data...\n",
      "2025-01-06 00:11:35,454 - INFO - Exporting validation data...\n",
      "2025-01-06 00:11:35,454 - INFO - Exporting validation data...\n",
      "2025-01-06 00:11:35,454 - INFO - Exporting validation data...\n",
      "2025-01-06 00:11:35,454 - INFO - Exporting validation data...\n",
      "2025-01-06 00:11:35,454 - INFO - Exporting validation data...\n",
      "2025-01-06 00:11:35,454 - INFO - Exporting validation data...\n",
      "2025-01-06 00:11:35,454 - INFO - Exporting validation data...\n",
      "2025-01-06 00:11:35,454 - INFO - Exporting validation data...\n",
      "2025-01-06 00:11:35,454 - INFO - Exporting validation data...\n",
      "2025-01-06 00:11:35,454 - INFO - Exporting validation data...\n",
      "2025-01-06 00:11:35,454 - INFO - Exporting validation data...\n"
     ]
    }
   ],
   "source": [
    "logger.info('Exporting validation data...')\n",
    "get_set_dataframe(\n",
    "    dict(ratio_split.val_set.user_data),\n",
    "    list(ratio_split.val_set.item_ids),\n",
    "    list(ratio_split.val_set.user_ids),\n",
    ").to_csv(results_dir / 'val_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc57886ed53253",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And we run the experiments with the defined variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "eee47a93bb6b755c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 00:11:37,553 - INFO - Running experiment...\n",
      "2025-01-06 00:11:37,553 - INFO - Running experiment...\n",
      "2025-01-06 00:11:37,553 - INFO - Running experiment...\n",
      "2025-01-06 00:11:37,553 - INFO - Running experiment...\n",
      "2025-01-06 00:11:37,553 - INFO - Running experiment...\n",
      "2025-01-06 00:11:37,553 - INFO - Running experiment...\n",
      "2025-01-06 00:11:37,553 - INFO - Running experiment...\n",
      "2025-01-06 00:11:37,553 - INFO - Running experiment...\n",
      "2025-01-06 00:11:37,553 - INFO - Running experiment...\n",
      "2025-01-06 00:11:37,553 - INFO - Running experiment...\n",
      "2025-01-06 00:11:37,553 - INFO - Running experiment...\n",
      "2025-01-06 00:11:37,553 - INFO - Running experiment...\n",
      "2025-01-06 00:11:37,553 - INFO - Running experiment...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[neumf_default] Training started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:31<00:00,  4.57s/it, loss=0.0671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[neumf_default] Evaluation started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking: 100%|██████████| 1262/1262 [00:10<00:00, 125.97it/s]\n",
      "Ranking: 100%|██████████| 1219/1219 [00:09<00:00, 132.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[neumf_deep] Training started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [10:08<00:00, 12.17s/it, loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[neumf_deep] Evaluation started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking: 100%|██████████| 1262/1262 [00:17<00:00, 71.39it/s]\n",
      "Ranking: 100%|██████████| 1219/1219 [00:17<00:00, 70.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[neumf_fair] Training started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [04:15<00:00,  8.50s/it, loss=0.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[neumf_fair] Evaluation started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking: 100%|██████████| 1262/1262 [00:37<00:00, 33.71it/s]\n",
      "Ranking: 100%|██████████| 1219/1219 [00:36<00:00, 33.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALIDATION:\n",
      "...\n",
      "              | NDCG@1000 | Precision@1000 | Recall@1000 | Time (s)\n",
      "------------- + --------- + -------------- + ----------- + --------\n",
      "neumf_default |    0.1828 |         0.0038 |      0.5194 |   9.2150\n",
      "neumf_deep    |    0.0060 |         0.0002 |      0.0279 |  17.3480\n",
      "neumf_fair    |    0.0060 |         0.0002 |      0.0279 |  36.0240\n",
      "\n",
      "TEST:\n",
      "...\n",
      "              | NDCG@1000 | Precision@1000 | Recall@1000 | Train (s) | Test (s)\n",
      "------------- + --------- + -------------- + ----------- + --------- + --------\n",
      "neumf_default |    0.2328 |         0.0075 |      0.5145 |   92.0966 |  10.0270\n",
      "neumf_deep    |    0.0075 |         0.0003 |      0.0273 |  608.9439 |  17.6795\n",
      "neumf_fair    |    0.0075 |         0.0003 |      0.0273 |  255.4582 |  37.4410\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info('Running experiment...')\n",
    "exp = Experiment(\n",
    "    eval_method=ratio_split,\n",
    "    models=models,\n",
    "    metrics=metrics,\n",
    "    user_based=True,\n",
    ")\n",
    "exp.run()\n",
    "\n",
    "#print(rs_mcf.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faff40fa70f6cf3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After running the experiment, we export the metrics obtained from the calculation into a csv using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "4b07b42b349bb38a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 00:29:41,842 - INFO - Exporting metrics...\n",
      "2025-01-06 00:29:41,842 - INFO - Exporting metrics...\n",
      "2025-01-06 00:29:41,842 - INFO - Exporting metrics...\n",
      "2025-01-06 00:29:41,842 - INFO - Exporting metrics...\n",
      "2025-01-06 00:29:41,842 - INFO - Exporting metrics...\n",
      "2025-01-06 00:29:41,842 - INFO - Exporting metrics...\n",
      "2025-01-06 00:29:41,842 - INFO - Exporting metrics...\n",
      "2025-01-06 00:29:41,842 - INFO - Exporting metrics...\n",
      "2025-01-06 00:29:41,842 - INFO - Exporting metrics...\n",
      "2025-01-06 00:29:41,842 - INFO - Exporting metrics...\n",
      "2025-01-06 00:29:41,842 - INFO - Exporting metrics...\n",
      "2025-01-06 00:29:41,842 - INFO - Exporting metrics...\n",
      "2025-01-06 00:29:41,842 - INFO - Exporting metrics...\n"
     ]
    }
   ],
   "source": [
    "logger.info('Exporting metrics...')\n",
    "metric_results = {\n",
    "    exp.models[i].name: dict(exp.result[i].metric_avg_results)\n",
    "    for i in range(len(models))\n",
    "}\n",
    "(DataFrame(metric_results)\n",
    " .reset_index()\n",
    " .rename(columns={'index': 'metric'})\n",
    " .to_csv(results_dir / 'metric_results.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4ad9fa00e924ff",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And finally we export the recommendations. We use a custom multi loop to get the results.\n",
    "- Here we first loop over the models of the experiment.\n",
    "- We loop over the users map of cornac to get both the original id and the internal index of cornac.\n",
    "- We get the scores for the users.\n",
    "- We get the k top items using a combination of argsort and reversing of the list.\n",
    "- We loop over the items map of cornac to get both the original id and the internal index of cornac.\n",
    "- We get the score obtained from cornac, or nan in case of IndexError.\n",
    "- We append the user and items, both the id and indexes, and the score to the result list.\n",
    "- After all the loops are finished, we export the data into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "fa3b156d0a731c3e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 00:29:41,873 - INFO - Processing models...\n",
      "2025-01-06 00:29:41,873 - INFO - Processing models...\n",
      "2025-01-06 00:29:41,873 - INFO - Processing models...\n",
      "2025-01-06 00:29:41,873 - INFO - Processing models...\n",
      "2025-01-06 00:29:41,873 - INFO - Processing models...\n",
      "2025-01-06 00:29:41,873 - INFO - Processing models...\n",
      "2025-01-06 00:29:41,873 - INFO - Processing models...\n",
      "2025-01-06 00:29:41,873 - INFO - Processing models...\n",
      "2025-01-06 00:29:41,873 - INFO - Processing models...\n",
      "2025-01-06 00:29:41,873 - INFO - Processing models...\n",
      "2025-01-06 00:29:41,873 - INFO - Processing models...\n",
      "2025-01-06 00:29:41,873 - INFO - Processing models...\n",
      "2025-01-06 00:29:41,873 - INFO - Processing models...\n",
      "2025-01-06 00:29:42,206 - INFO - Getting scores for neumf_default...\n",
      "2025-01-06 00:29:42,206 - INFO - Getting scores for neumf_default...\n",
      "2025-01-06 00:29:42,206 - INFO - Getting scores for neumf_default...\n",
      "2025-01-06 00:29:42,206 - INFO - Getting scores for neumf_default...\n",
      "2025-01-06 00:29:42,206 - INFO - Getting scores for neumf_default...\n",
      "2025-01-06 00:29:42,206 - INFO - Getting scores for neumf_default...\n",
      "2025-01-06 00:29:42,206 - INFO - Getting scores for neumf_default...\n",
      "2025-01-06 00:29:42,206 - INFO - Getting scores for neumf_default...\n",
      "2025-01-06 00:29:42,206 - INFO - Getting scores for neumf_default...\n",
      "2025-01-06 00:29:42,206 - INFO - Getting scores for neumf_default...\n",
      "2025-01-06 00:29:42,206 - INFO - Getting scores for neumf_default...\n",
      "2025-01-06 00:29:42,206 - INFO - Getting scores for neumf_default...\n",
      "2025-01-06 00:29:42,206 - INFO - Getting scores for neumf_default...\n",
      "100%|██████████| 1288/1288 [24:34<00:00,  1.14s/it]\n",
      "2025-01-06 00:54:16,283 - INFO - Exporting neumf_default...\n",
      "2025-01-06 00:54:16,283 - INFO - Exporting neumf_default...\n",
      "2025-01-06 00:54:16,283 - INFO - Exporting neumf_default...\n",
      "2025-01-06 00:54:16,283 - INFO - Exporting neumf_default...\n",
      "2025-01-06 00:54:16,283 - INFO - Exporting neumf_default...\n",
      "2025-01-06 00:54:16,283 - INFO - Exporting neumf_default...\n",
      "2025-01-06 00:54:16,283 - INFO - Exporting neumf_default...\n",
      "2025-01-06 00:54:16,283 - INFO - Exporting neumf_default...\n",
      "2025-01-06 00:54:16,283 - INFO - Exporting neumf_default...\n",
      "2025-01-06 00:54:16,283 - INFO - Exporting neumf_default...\n",
      "2025-01-06 00:54:16,283 - INFO - Exporting neumf_default...\n",
      "2025-01-06 00:54:16,283 - INFO - Exporting neumf_default...\n",
      "2025-01-06 00:54:16,283 - INFO - Exporting neumf_default...\n",
      "2025-01-06 00:54:36,430 - INFO - Getting scores for neumf_deep...\n",
      "2025-01-06 00:54:36,430 - INFO - Getting scores for neumf_deep...\n",
      "2025-01-06 00:54:36,430 - INFO - Getting scores for neumf_deep...\n",
      "2025-01-06 00:54:36,430 - INFO - Getting scores for neumf_deep...\n",
      "2025-01-06 00:54:36,430 - INFO - Getting scores for neumf_deep...\n",
      "2025-01-06 00:54:36,430 - INFO - Getting scores for neumf_deep...\n",
      "2025-01-06 00:54:36,430 - INFO - Getting scores for neumf_deep...\n",
      "2025-01-06 00:54:36,430 - INFO - Getting scores for neumf_deep...\n",
      "2025-01-06 00:54:36,430 - INFO - Getting scores for neumf_deep...\n",
      "2025-01-06 00:54:36,430 - INFO - Getting scores for neumf_deep...\n",
      "2025-01-06 00:54:36,430 - INFO - Getting scores for neumf_deep...\n",
      "2025-01-06 00:54:36,430 - INFO - Getting scores for neumf_deep...\n",
      "2025-01-06 00:54:36,430 - INFO - Getting scores for neumf_deep...\n",
      "100%|██████████| 1288/1288 [24:15<00:00,  1.13s/it]\n",
      "2025-01-06 01:18:52,183 - INFO - Exporting neumf_deep...\n",
      "2025-01-06 01:18:52,183 - INFO - Exporting neumf_deep...\n",
      "2025-01-06 01:18:52,183 - INFO - Exporting neumf_deep...\n",
      "2025-01-06 01:18:52,183 - INFO - Exporting neumf_deep...\n",
      "2025-01-06 01:18:52,183 - INFO - Exporting neumf_deep...\n",
      "2025-01-06 01:18:52,183 - INFO - Exporting neumf_deep...\n",
      "2025-01-06 01:18:52,183 - INFO - Exporting neumf_deep...\n",
      "2025-01-06 01:18:52,183 - INFO - Exporting neumf_deep...\n",
      "2025-01-06 01:18:52,183 - INFO - Exporting neumf_deep...\n",
      "2025-01-06 01:18:52,183 - INFO - Exporting neumf_deep...\n",
      "2025-01-06 01:18:52,183 - INFO - Exporting neumf_deep...\n",
      "2025-01-06 01:18:52,183 - INFO - Exporting neumf_deep...\n",
      "2025-01-06 01:18:52,183 - INFO - Exporting neumf_deep...\n",
      "2025-01-06 01:19:12,226 - INFO - Getting scores for neumf_fair...\n",
      "2025-01-06 01:19:12,226 - INFO - Getting scores for neumf_fair...\n",
      "2025-01-06 01:19:12,226 - INFO - Getting scores for neumf_fair...\n",
      "2025-01-06 01:19:12,226 - INFO - Getting scores for neumf_fair...\n",
      "2025-01-06 01:19:12,226 - INFO - Getting scores for neumf_fair...\n",
      "2025-01-06 01:19:12,226 - INFO - Getting scores for neumf_fair...\n",
      "2025-01-06 01:19:12,226 - INFO - Getting scores for neumf_fair...\n",
      "2025-01-06 01:19:12,226 - INFO - Getting scores for neumf_fair...\n",
      "2025-01-06 01:19:12,226 - INFO - Getting scores for neumf_fair...\n",
      "2025-01-06 01:19:12,226 - INFO - Getting scores for neumf_fair...\n",
      "2025-01-06 01:19:12,226 - INFO - Getting scores for neumf_fair...\n",
      "2025-01-06 01:19:12,226 - INFO - Getting scores for neumf_fair...\n",
      "2025-01-06 01:19:12,226 - INFO - Getting scores for neumf_fair...\n",
      "100%|██████████| 1288/1288 [24:17<00:00,  1.13s/it]\n",
      "2025-01-06 01:43:29,827 - INFO - Exporting neumf_fair...\n",
      "2025-01-06 01:43:29,827 - INFO - Exporting neumf_fair...\n",
      "2025-01-06 01:43:29,827 - INFO - Exporting neumf_fair...\n",
      "2025-01-06 01:43:29,827 - INFO - Exporting neumf_fair...\n",
      "2025-01-06 01:43:29,827 - INFO - Exporting neumf_fair...\n",
      "2025-01-06 01:43:29,827 - INFO - Exporting neumf_fair...\n",
      "2025-01-06 01:43:29,827 - INFO - Exporting neumf_fair...\n",
      "2025-01-06 01:43:29,827 - INFO - Exporting neumf_fair...\n",
      "2025-01-06 01:43:29,827 - INFO - Exporting neumf_fair...\n",
      "2025-01-06 01:43:29,827 - INFO - Exporting neumf_fair...\n",
      "2025-01-06 01:43:29,827 - INFO - Exporting neumf_fair...\n",
      "2025-01-06 01:43:29,827 - INFO - Exporting neumf_fair...\n",
      "2025-01-06 01:43:29,827 - INFO - Exporting neumf_fair...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "logger.info('Processing models...')\n",
    "for model in exp.models:\n",
    "    model_result = []\n",
    "    logger.info(f'Getting scores for {model.name}...')\n",
    "\n",
    "    for user_id, user_index in tqdm(exp.eval_method.train_set.uid_map.items()):\n",
    "        try:\n",
    "            scores = model.score(user_index)\n",
    "        except ScoreException:\n",
    "            logger.error(f\"{model.name}: Couldn't predict for user {user_index} ({user_id=})\")\n",
    "            continue\n",
    "\n",
    "        top_items = list(reversed(scores.argsort()))[:k]\n",
    "\n",
    "        for item_id, item_index in exp.eval_method.train_set.iid_map.items():\n",
    "            if item_index not in top_items:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                score = scores[item_index]\n",
    "            except IndexError:\n",
    "                logger.error(\n",
    "                    f\"{model.name}: No score for item {item_index} ({item_id=}) in user {user_index} ({user_id=})\"\n",
    "                )\n",
    "                score = nan\n",
    "\n",
    "            model_result.append({\n",
    "                'user_id': user_id,\n",
    "                'user_index': user_index,\n",
    "                'item_id': item_id,\n",
    "                'item_index': item_index,\n",
    "                'score': score\n",
    "            })\n",
    "\n",
    "    logger.info(f'Exporting {model.name}...')\n",
    "    (DataFrame(model_result)\n",
    "     .sort_values(by=['user_id', 'score'], ascending=[True, False])\n",
    "     .to_csv(results_dir / f'{model.name}.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
