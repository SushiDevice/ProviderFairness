{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69ba81caf58e1381",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Imports\n",
    "These experiments were run on Python 3.8. In the requirements.txt are the versions used for these packages.\n",
    "- tqdm: For showing progress in loops.\n",
    "- numpy and pandas: For data manipulation.\n",
    "- cornac: For obtaining the recommendations.\n",
    "- tensorflow: Required by cornac.\n",
    "- torch: Required for the VAECF implementation of Cornac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from logging import Formatter, StreamHandler, getLogger, INFO\n",
    "\n",
    "from tqdm import tqdm\n",
    "from cornac import Experiment\n",
    "from cornac.eval_methods import RatioSplit, BaseMethod\n",
    "from cornac.metrics import NDCG, Recall, Precision\n",
    "from cornac.models import MF, WMF, SVD, VAECF, UserKNN\n",
    "from cornac.exception import ScoreException\n",
    "from numpy import array, nan\n",
    "from pandas import read_csv, DataFrame, Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419102f5de0b9dd3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Logger setup\n",
    "Here we set up the logger for showing some info when executing this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2daab4dcf47f0e24",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger = getLogger(__name__)\n",
    "logger.setLevel(INFO)\n",
    "\n",
    "ch = StreamHandler()\n",
    "ch.setLevel(INFO)\n",
    "ch.setFormatter(\n",
    "    Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    ")\n",
    "\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c488da52c0b011",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Configuration variables\n",
    "We define some variables used on the rest of the experiment.\n",
    "\n",
    "### General config\n",
    "Getting the date now and the name of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d635b3d3d5b23e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "now = f'{datetime.now():%Y%m%d%H%M%S}'\n",
    "experiment_name = 'AMBAR'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c6a66141bf440",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### File and dir config\n",
    "Getting the working directory with pathlib, and obtaining the csv to be used in cornac, and defining a results directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2b5c46e25bce6b97",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Work directory\n",
    "work_dir = Path('.').resolve()\n",
    "\n",
    "# Resuls directory, scores will be saved here\n",
    "results_dir = work_dir / 'results' / now\n",
    "\n",
    "# preprocessing data directory\n",
    "prep = work_dir / 'data' / 'PFair' / 'Preprocessing'\n",
    "\n",
    "# preprocessing train data\n",
    "train_set = prep / 'train_filtered2.csv'\n",
    "\n",
    "# preprocessing test data\n",
    "test_set = prep / 'test_filtered.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51611283d1977550",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here we make sure the results directory exists by creating it if it doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a635f7c30ba080d2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If directory doesnt existe, create one\n",
    "if not results_dir.exists():\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaad9a156a91656b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Also, we make sure the data file exists and is a file. Here we could also make sure that the file is an actual csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d97ea640067e44fd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (not train_set.exists() and train_set.is_file()) or (not train_set.exists() and train_set.is_file()) :\n",
    "    print(\"Bad data file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5bb98578bc357",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Dataframe config\n",
    "We define the names of the headers of each column to be identified by pandas. Also, we define the data type of the values in each cell of the user, item and rating. If the data has multiple data types, the val_dtype can be a list of type string compatible with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "79e39b6dbc03ed72",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Establish columns names for using later\n",
    "col_names = {\n",
    "    'user': 'user_id',\n",
    "    'item': 'track_id',\n",
    "    'rating': 'rating'\n",
    "}\n",
    "val_dtype = 'int'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f3e896f286446",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Cornac config\n",
    "Here we set up the k value, the test set size and the validation set size. Also we decide if we want to exclude unknown values or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f569e7c7b4df6096",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 1000\n",
    "#test_size = 0.2\n",
    "#val_size = 0.1\n",
    "exclude_unknown = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab5851fd6f35830",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Function setup\n",
    "We set up various utility functions to be used later. Mostly for exporting data and getting it in a format compatible with cornac.\n",
    "\n",
    "set_data_to_tuple_list takes a dict of {user: [item_list, rating_list]}, process it and returns a tuple list of format [(user, item, rating)...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3f809e7ca6f8cedc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_data_to_tuple_list(d: dict) -> list:\n",
    "    result = []\n",
    "    for user in d:\n",
    "        transpose = array(d[user]).T\n",
    "        for t in transpose:\n",
    "            result.append((user,) + tuple(t))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5bd6bbdad0dba2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "list_to_dict converts a list into a dict using dict comprehension and enumerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "692ef4f13a3a5c51",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def list_to_dict(l: list) -> dict:\n",
    "    return {i: v for i, v in enumerate(l)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b75cb4025f5f7fa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "get_set_dataframe process the raw data ({user: [item_list, rating_list]}), with the item ids and user ids, and converts it into a pandas DataFrame to be exported later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d0f5aba5f16befe8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_set_dataframe(set_data: dict, i_ids: list, u_ids: list) -> DataFrame:\n",
    "    data_list = set_data_to_tuple_list(set_data)\n",
    "    i_map = list_to_dict(i_ids)\n",
    "    u_map = list_to_dict(u_ids)\n",
    "\n",
    "    set_df = DataFrame(data_list,\n",
    "                       columns=list(col_names.values()),\n",
    "                       dtype=val_dtype)\n",
    "    set_df['item_idx'] = set_df[col_names['item']]\n",
    "    set_df['item'] = set_df[col_names['item']].replace(to_replace=i_map)\n",
    "    set_df['user_idx'] = set_df[col_names['user']]\n",
    "    set_df['user'] = set_df[col_names['user']].replace(to_replace=u_map)\n",
    "    return set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c2f3457a3965bf91",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-12 23:32:10,342 - INFO - Experiment start...\n",
      "2025-01-12 23:32:10,342 - INFO - Experiment start...\n",
      "2025-01-12 23:32:10,342 - INFO - Experiment start...\n",
      "2025-01-12 23:32:10,342 - INFO - Experiment start...\n",
      "2025-01-12 23:32:10,342 - INFO - Experiment start...\n",
      "2025-01-12 23:32:10,342 - INFO - Experiment start...\n",
      "2025-01-12 23:32:10,342 - INFO - Experiment start...\n",
      "2025-01-12 23:32:10,342 - INFO - Experiment start...\n",
      "2025-01-12 23:32:10,346 - INFO - AMBAR\n",
      "2025-01-12 23:32:10,346 - INFO - AMBAR\n",
      "2025-01-12 23:32:10,346 - INFO - AMBAR\n",
      "2025-01-12 23:32:10,346 - INFO - AMBAR\n",
      "2025-01-12 23:32:10,346 - INFO - AMBAR\n",
      "2025-01-12 23:32:10,346 - INFO - AMBAR\n",
      "2025-01-12 23:32:10,346 - INFO - AMBAR\n",
      "2025-01-12 23:32:10,346 - INFO - AMBAR\n",
      "2025-01-12 23:32:10,350 - INFO - k=1000\n",
      "2025-01-12 23:32:10,350 - INFO - k=1000\n",
      "2025-01-12 23:32:10,350 - INFO - k=1000\n",
      "2025-01-12 23:32:10,350 - INFO - k=1000\n",
      "2025-01-12 23:32:10,350 - INFO - k=1000\n",
      "2025-01-12 23:32:10,350 - INFO - k=1000\n",
      "2025-01-12 23:32:10,350 - INFO - k=1000\n",
      "2025-01-12 23:32:10,350 - INFO - k=1000\n",
      "2025-01-12 23:32:10,354 - INFO - work_dir=WindowsPath('M:/Framework/AMBAR')\n",
      "2025-01-12 23:32:10,354 - INFO - work_dir=WindowsPath('M:/Framework/AMBAR')\n",
      "2025-01-12 23:32:10,354 - INFO - work_dir=WindowsPath('M:/Framework/AMBAR')\n",
      "2025-01-12 23:32:10,354 - INFO - work_dir=WindowsPath('M:/Framework/AMBAR')\n",
      "2025-01-12 23:32:10,354 - INFO - work_dir=WindowsPath('M:/Framework/AMBAR')\n",
      "2025-01-12 23:32:10,354 - INFO - work_dir=WindowsPath('M:/Framework/AMBAR')\n",
      "2025-01-12 23:32:10,354 - INFO - work_dir=WindowsPath('M:/Framework/AMBAR')\n",
      "2025-01-12 23:32:10,354 - INFO - work_dir=WindowsPath('M:/Framework/AMBAR')\n",
      "2025-01-12 23:32:10,358 - INFO - results_dir=WindowsPath('M:/Framework/AMBAR/results/20250112233210')\n",
      "2025-01-12 23:32:10,358 - INFO - results_dir=WindowsPath('M:/Framework/AMBAR/results/20250112233210')\n",
      "2025-01-12 23:32:10,358 - INFO - results_dir=WindowsPath('M:/Framework/AMBAR/results/20250112233210')\n",
      "2025-01-12 23:32:10,358 - INFO - results_dir=WindowsPath('M:/Framework/AMBAR/results/20250112233210')\n",
      "2025-01-12 23:32:10,358 - INFO - results_dir=WindowsPath('M:/Framework/AMBAR/results/20250112233210')\n",
      "2025-01-12 23:32:10,358 - INFO - results_dir=WindowsPath('M:/Framework/AMBAR/results/20250112233210')\n",
      "2025-01-12 23:32:10,358 - INFO - results_dir=WindowsPath('M:/Framework/AMBAR/results/20250112233210')\n",
      "2025-01-12 23:32:10,358 - INFO - results_dir=WindowsPath('M:/Framework/AMBAR/results/20250112233210')\n"
     ]
    }
   ],
   "source": [
    "logger.info('Experiment start...')\n",
    "logger.info(f'{experiment_name}')\n",
    "logger.info(f'{k=}')\n",
    "logger.info(f'{work_dir=}')\n",
    "logger.info(f'{results_dir=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7017668614f3b926",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here we create the dataset out of the data file, the expected data is only with user, item and rating in that order. The name of the columns is defined in the set-up part, same with the data types.\n",
    "\n",
    "For testing purposes before actually executing the full experiment, we left a filter that takes a sample of 50 users, and gets only the data of those 50 users. Please use it only to make sure that the script executes correctly from start to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d16d3a7ebfcee39b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"keys = ['0', '1', '2']\\n\\nif isinstance(val_dtype, str):\\n    d_type = {key: val_dtype for key in keys}\\nelif isinstance(val_dtype, list):\\n    d_type = dict(zip(keys, val_dtype))\\nelse:\\n    logger.error('Wrong type setup. Must be a type string or a list of type string.')\\n    exit()\\n\\nlogger.info('Loading data into triplets...')\\ndf = read_csv(\\n    data_file,\\n    header=0,\\n    names=['0', '1', '2']\\n)[['0', '1', '2']].astype(d_type)\\n\\n# FOR TESTING ONLY\\nuser_filter = Series(df['0'].unique()).sample(50, random_state=123).to_list()\\ndf = df[df['0'].isin(user_filter)]\\nprint(df.head())\\ndata = list(df.to_records(index=False, column_dtypes=d_type))\""
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"keys = ['0', '1', '2']\n",
    "\n",
    "if isinstance(val_dtype, str):\n",
    "    d_type = {key: val_dtype for key in keys}\n",
    "elif isinstance(val_dtype, list):\n",
    "    d_type = dict(zip(keys, val_dtype))\n",
    "else:\n",
    "    logger.error('Wrong type setup. Must be a type string or a list of type string.')\n",
    "    exit()\n",
    "\n",
    "logger.info('Loading data into triplets...')\n",
    "df = read_csv(\n",
    "    data_file,\n",
    "    header=0,\n",
    "    names=['0', '1', '2']\n",
    ")[['0', '1', '2']].astype(d_type)\n",
    "\n",
    "# FOR TESTING ONLY\n",
    "user_filter = Series(df['0'].unique()).sample(50, random_state=123).to_list()\n",
    "df = df[df['0'].isin(user_filter)]\n",
    "print(df.head())\n",
    "data = list(df.to_records(index=False, column_dtypes=d_type))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d232af26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-12 23:32:10,392 - INFO - Loading data into triplets...\n",
      "2025-01-12 23:32:10,392 - INFO - Loading data into triplets...\n",
      "2025-01-12 23:32:10,392 - INFO - Loading data into triplets...\n",
      "2025-01-12 23:32:10,392 - INFO - Loading data into triplets...\n",
      "2025-01-12 23:32:10,392 - INFO - Loading data into triplets...\n",
      "2025-01-12 23:32:10,392 - INFO - Loading data into triplets...\n",
      "2025-01-12 23:32:10,392 - INFO - Loading data into triplets...\n",
      "2025-01-12 23:32:10,392 - INFO - Loading data into triplets...\n"
     ]
    }
   ],
   "source": [
    "# PARA TESTEAR\n",
    "# Here we change the input data format to be compatible with cornac evaluation method\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "keys = ['0', '1', '2']\n",
    "\n",
    "if isinstance(val_dtype, str):\n",
    "    d_type = {key: val_dtype for key in keys}\n",
    "elif isinstance(val_dtype, list):\n",
    "    d_type = dict(zip(keys, val_dtype))\n",
    "else:\n",
    "    logger.error('Wrong type setup. Must be a type string or a list of type string.')\n",
    "    exit()\n",
    "# We change the columns names for the train set\n",
    "logger.info('Loading data into triplets...')\n",
    "df = read_csv(\n",
    "    train_set,\n",
    "    header=0,\n",
    "    names=['0', '1', '2']\n",
    ")[['0', '1', '2']].astype(d_type)\n",
    "# We change the columns names for the test set\n",
    "df1 = read_csv(\n",
    "    test_set,\n",
    "    header=0,\n",
    "    names=['0', '1', '2']\n",
    ")[['0', '1', '2']].astype(d_type)\n",
    "\n",
    "# If validation data was needed, repeat the code here\n",
    "\n",
    "\n",
    "# FOR TESTING ONLY \n",
    "    # THIS WILL GENERATE 500 RANDOM USERS FOR SAMPLING \n",
    "\n",
    "# Increase the number of users sampled\n",
    "#user_filter = Series(df['0'].unique()).sample(500, random_state=333).to_list()  # Increase from 200 to 500\n",
    "#df = df[df['0'].isin(user_filter)]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "#train_df, test_df = train_test_split(df, test_size=0.2, random_state=333, stratify=df['2'])  # Stratify by ratings\n",
    "\n",
    "#train_data = list(train_df.to_records(index=False, column_dtypes=d_type))\n",
    "#test_data = list(test_df.to_records(index=False, column_dtypes=d_type))\n",
    "\n",
    "# FOR TESTING ONLY\n",
    "\n",
    "\"\"\"print(\"Training data:\")\n",
    "print(df.head)\n",
    "print(\"Test data:\")\n",
    "print(df1.head)\"\"\"\n",
    "\n",
    "train_data = list(df.to_records(index=False, column_dtypes=d_type))\n",
    "test_data = list(df1.to_records(index=False, column_dtypes=d_type))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300bbb3260fd4305",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here we create the Ratio Split that will be used by cornac. It splits the data into 3 sets randomly. 1 for test, 1 for train and 1 for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c9e6e383e56136f1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating_threshold = 1.0\n",
      "exclude_unknowns = True\n",
      "---\n",
      "Training data:\n",
      "Number of users = 500\n",
      "Number of items = 3723\n",
      "Number of ratings = 8411\n",
      "Max rating = 5.0\n",
      "Min rating = 1.0\n",
      "Global mean = 1.7\n",
      "---\n",
      "Test data:\n",
      "Number of users = 500\n",
      "Number of items = 3723\n",
      "Number of ratings = 1678\n",
      "Number of unknown users = 0\n",
      "Number of unknown items = 0\n",
      "---\n",
      "Total users = 500\n",
      "Total items = 3723\n",
      "probando\n",
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Juan Jose\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cornac\\data\\dataset.py:335: UserWarning: 2 duplicated observations are removed!\n",
      "  warnings.warn(\"%d duplicated observations are removed!\" % dup_count)\n"
     ]
    }
   ],
   "source": [
    "# Here we stablish the evaluation method, it can be ratio split\n",
    "# cross/validation, stratified, etc. the base method of this class\n",
    "# allows to give or owns set, but must be change the format like we did in above cell\n",
    "# ------------------- //Check Cornac documentation// -------------------\n",
    "\n",
    "eval_method = BaseMethod.from_splits(\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    val_data=None,  # Optional, can be omitted if not using a validation set\n",
    "    #fmt='UIR',  # Format of the data, typically 'UIR' for user-item-rating\n",
    "    rating_threshold=1.0,  # Threshold for considering a rating as positive\n",
    "    exclude_unknowns=True,  # Whether to exclude unknown users/items\n",
    "    verbose=True  # Whether to print detailed logs\n",
    ")\n",
    "\n",
    "# Probando (ignorar)\n",
    "#user_data = eval_method.test_set.num_users\n",
    "                          \n",
    "#print(\"probando\")\n",
    "#print(user_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab41cdeeccb9adf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We define the metris here. In this experiment, we set up NDCG, Recall and Precision, using the k defined in the set-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "34a2aa4217a63dfc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we create a list to give to the experiment further in the code.\n",
    "# More metrics can be exported in the first cell.\n",
    "# ------------------- //Check Cornac documentation for more metrics// -------------------\n",
    "\n",
    "metrics = [\n",
    "    NDCG(k),\n",
    "    Recall(k),\n",
    "    Precision(k)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e4a74ce9da083",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Also, we define the models with some previously obtained parameters. We could also define the hyperparameter calculation in this part, in this case, is important to leave a models variable with said configuration, so cornac can pick up the array and execute the calculation and exporting of the recommendations.\n",
    "\n",
    "Because this script is assuming an array with models with parameters already predefined, in case of needing the best parameters obtained by cornac, the exporting of this must be done after running the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d689a754ab1d8510",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we save the models parameters for the training\n",
    "# models, as with metrics, must be imported from cornac\n",
    "# we save the models in variables to give as a list to\n",
    "# the experiment class further in the code\n",
    "# you may experiment with gridsearch and random search for tunning\n",
    "# ------------------- //Check Cornac documentation// -------------------\n",
    "\n",
    "from cornac.models import BiVAECF\n",
    "from cornac.hyperopt import Discrete, Continuous\n",
    "from cornac.hyperopt import GridSearch, RandomSearch\n",
    "\n",
    "biVAECF = BiVAECF(\n",
    "    name='vibae_exp1',\n",
    "    k=k,\n",
    "    encoder_structure=[20],\n",
    "    act_fn=\"tanh\",\n",
    "    likelihood=\"pois\",\n",
    "    n_epochs=100,\n",
    "    batch_size=100,\n",
    "    learning_rate=0.001,\n",
    "    beta_kl=1.0,\n",
    "    use_gpu=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# First alternative parameter set\n",
    "biVAECF_alt1 = BiVAECF(\n",
    "    name='bivae_exp2',\n",
    "    k=500,  # Reduced number of latent factors\n",
    "    encoder_structure=[50, 30],  # Different autoencoder structure\n",
    "    act_fn=\"relu\",  # Changed activation function\n",
    "    likelihood=\"gaus\",  # Changed likelihood\n",
    "    n_epochs=150,  # Increased number of epochs\n",
    "    batch_size=50,  # Smaller batch size\n",
    "    learning_rate=0.005,  # Increased learning rate\n",
    "    beta_kl=0.5,  # Reduced KL divergence weight\n",
    "    use_gpu=False,  # Disabled GPU usage\n",
    "    verbose=False  # Disabled verbosity\n",
    ")\n",
    "\n",
    "# Second alternative parameter set\n",
    "biVAECF_alt2 = BiVAECF(\n",
    "    name='vibae_exp3',\n",
    "    k=2000,  # Increased number of latent factors\n",
    "    encoder_structure=[100, 50, 25],  # More complex autoencoder structure\n",
    "    act_fn=\"sigmoid\",  # Different activation function\n",
    "    likelihood=\"bern\",  # Different likelihood\n",
    "    n_epochs=200,  # Further increased number of epochs\n",
    "    batch_size=200,  # Larger batch size\n",
    "    learning_rate=0.0001,  # Decreased learning rate\n",
    "    beta_kl=2.0,  # Increased KL divergence weight\n",
    "    use_gpu=True,  # Enabled GPU usage\n",
    "    verbose=True  # Enabled verbosity\n",
    ")\n",
    "\n",
    "gs_vae = GridSearch(\n",
    "    model=biVAECF,\n",
    "    space=[\n",
    "        Discrete(name=\"k\", values=[10,15,20,25]),\n",
    "        Discrete(name=\"encoder_structure\", values=[[20],]),\n",
    "        Discrete(name=\"n_epochs\", values=[100,150,200]),\n",
    "        Discrete(name=\"batch_size\", values=[100,150,200])\n",
    "    ],\n",
    "    metric=metrics,\n",
    "    eval_method=eval_method\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9d82ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we must pass the models variables were \n",
    "# we defined the parameters\n",
    "\n",
    "models = [ \n",
    "    #gs_vae\n",
    "    biVAECF,\n",
    "    #biVAECF_alt1,\n",
    "    #biVAECF_alt2\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e9d059a8baf22a2d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"total_users = ratio_split.train_set.num_users\\ntotal_items = ratio_split.train_set.num_items\\nlogger.info(f'{total_users=}')\\nlogger.info(f'{total_items=}')\""
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"total_users = ratio_split.train_set.num_users\n",
    "total_items = ratio_split.train_set.num_items\n",
    "logger.info(f'{total_users=}')\n",
    "logger.info(f'{total_items=}')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d75765eba73247",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After setting up the metrics and models, we export the test, train and validation data into the results directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cd3120168e041a6a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"logger.info('Exporting test data...')\\nget_set_dataframe(\\n    dict(ratio_split.test_set.user_data),\\n    list(ratio_split.test_set.item_ids),\\n    list(ratio_split.test_set.user_ids),\\n).to_csv(results_dir / 'test_set.csv')\""
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"logger.info('Exporting test data...')\n",
    "get_set_dataframe(\n",
    "    dict(ratio_split.test_set.user_data),\n",
    "    list(ratio_split.test_set.item_ids),\n",
    "    list(ratio_split.test_set.user_ids),\n",
    ").to_csv(results_dir / 'test_set.csv')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3cd0d1fd06dba550",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"logger.info('Exporting train data...')\\nget_set_dataframe(\\n    dict(ratio_split.train_set.user_data),\\n    list(ratio_split.train_set.item_ids),\\n    list(ratio_split.train_set.user_ids),\\n).to_csv(results_dir / 'train_set.csv')\""
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"logger.info('Exporting train data...')\n",
    "get_set_dataframe(\n",
    "    dict(ratio_split.train_set.user_data),\n",
    "    list(ratio_split.train_set.item_ids),\n",
    "    list(ratio_split.train_set.user_ids),\n",
    ").to_csv(results_dir / 'train_set.csv')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f6986140f5a050d1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"logger.info('Exporting validation data...')\\nget_set_dataframe(\\n    dict(ratio_split.val_set.user_data),\\n    list(ratio_split.val_set.item_ids),\\n    list(ratio_split.val_set.user_ids),\\n).to_csv(results_dir / 'val_set.csv')\""
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"logger.info('Exporting validation data...')\n",
    "get_set_dataframe(\n",
    "    dict(ratio_split.val_set.user_data),\n",
    "    list(ratio_split.val_set.item_ids),\n",
    "    list(ratio_split.val_set.user_ids),\n",
    ").to_csv(results_dir / 'val_set.csv')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc57886ed53253",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And we run the experiments with the defined variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "eee47a93bb6b755c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[vibae_exp1] Training started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:05<00:57,  1.58it/s, loss_i=0.895, loss_u=7.2] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[180], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run the experiment\u001b[39;00m\n\u001b[0;32m      2\u001b[0m exp \u001b[38;5;241m=\u001b[39m Experiment(\n\u001b[0;32m      3\u001b[0m     eval_method\u001b[38;5;241m=\u001b[39m eval_method,\n\u001b[0;32m      4\u001b[0m     models\u001b[38;5;241m=\u001b[39mmodels,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     )\n\u001b[1;32m----> 9\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m train_data \u001b[38;5;241m=\u001b[39m eval_method\u001b[38;5;241m.\u001b[39mtrain_set\n\u001b[0;32m     12\u001b[0m train_results \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mevaluate(train_data, metrics)\n",
      "File \u001b[1;32mc:\\Users\\Juan Jose\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cornac\\experiment\\experiment.py:142\u001b[0m, in \u001b[0;36mExperiment.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    139\u001b[0m         model\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels:\n\u001b[1;32m--> 142\u001b[0m     test_result, val_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_based\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_based\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mappend(test_result)\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Juan Jose\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cornac\\eval_methods\\base_method.py:734\u001b[0m, in \u001b[0;36mBaseMethod.evaluate\u001b[1;34m(self, model, metrics, user_based, show_validation)\u001b[0m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] Training started!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m    733\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 734\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    735\u001b[0m train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m    737\u001b[0m \u001b[38;5;66;03m##############\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;66;03m# EVALUATION #\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;66;03m##############\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Juan Jose\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cornac\\models\\bivaecf\\recom_bivaecf.py:178\u001b[0m, in \u001b[0;36mBiVAECF.fit\u001b[1;34m(self, train_set, val_set)\u001b[0m\n\u001b[0;32m    166\u001b[0m         num_users \u001b[38;5;241m=\u001b[39m train_set\u001b[38;5;241m.\u001b[39mmatrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbivae \u001b[38;5;241m=\u001b[39m BiVAE(\n\u001b[0;32m    168\u001b[0m             k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk,\n\u001b[0;32m    169\u001b[0m             user_encoder_structure\u001b[38;5;241m=\u001b[39m[num_items] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_structure,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    175\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m    176\u001b[0m         )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 178\u001b[0m     \u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbivae\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearn_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta_kl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta_kl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is trained already (trainable = False)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n",
      "File \u001b[1;32mc:\\Users\\Juan Jose\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cornac\\models\\bivaecf\\bivae.py:205\u001b[0m, in \u001b[0;36mlearn\u001b[1;34m(bivae, train_set, n_epochs, batch_size, learn_rate, beta_kl, verbose, device, dtype)\u001b[0m\n\u001b[0;32m    202\u001b[0m i_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(i_batch, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# Reconstructed batch\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m beta, i_batch_, i_mu, i_std \u001b[38;5;241m=\u001b[39m \u001b[43mbivae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbivae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m i_mu_prior \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m  \u001b[38;5;66;03m# zero mean for standard normal prior if not CAP prior\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bivae\u001b[38;5;241m.\u001b[39mcap_priors\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Juan Jose\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cornac\\models\\bivaecf\\bivae.py:133\u001b[0m, in \u001b[0;36mBiVAE.forward\u001b[1;34m(self, x, user, beta, theta)\u001b[0m\n\u001b[0;32m    131\u001b[0m mu, std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_item(x)\n\u001b[0;32m    132\u001b[0m beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mu, std)\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m beta, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m, mu, std\n",
      "File \u001b[1;32mc:\\Users\\Juan Jose\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cornac\\models\\bivaecf\\bivae.py:118\u001b[0m, in \u001b[0;36mBiVAE.decode_item\u001b[1;34m(self, theta, beta)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, theta, beta):\n\u001b[0;32m    117\u001b[0m     h \u001b[38;5;241m=\u001b[39m beta\u001b[38;5;241m.\u001b[39mmm(theta\u001b[38;5;241m.\u001b[39mt())\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Here we run the experiment\n",
    "exp = Experiment(\n",
    "    eval_method= eval_method,\n",
    "    models=models,\n",
    "    metrics=metrics,\n",
    "    user_based=True,\n",
    "    verbose= True\n",
    "    )\n",
    "exp.run()\n",
    "\n",
    "train_data = eval_method.train_set\n",
    "train_results = models.evaluate(train_data, metrics)\n",
    "print(\"Training Results:\", train_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faff40fa70f6cf3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After running the experiment, we export the metrics obtained from the calculation into a csv using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b07b42b349bb38a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we export the metrics from the experiment in the test\n",
    "# and validation data (if exist)\n",
    "\n",
    "logger.info('Exporting metrics...')\n",
    "metric_results = {\n",
    "    exp.models[i].name: dict(exp.result[i].metric_avg_results)\n",
    "    for i in range(len(models))\n",
    "}\n",
    "(DataFrame(metric_results)\n",
    " .reset_index()\n",
    " .rename(columns={'index': 'metric'})\n",
    " .to_csv(results_dir / 'metric_results.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4ad9fa00e924ff",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And finally we export the recommendations. We use a custom multi loop to get the results.\n",
    "- Here we first loop over the models of the experiment.\n",
    "- We loop over the users map of cornac to get both the original id and the internal index of cornac.\n",
    "- We get the scores for the users.\n",
    "- We get the k top items using a combination of argsort and reversing of the list.\n",
    "- We loop over the items map of cornac to get both the original id and the internal index of cornac.\n",
    "- We get the score obtained from cornac, or nan in case of IndexError.\n",
    "- We append the user and items, both the id and indexes, and the score to the result list.\n",
    "- After all the loops are finished, we export the data into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3b156d0a731c3e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Export recommendations\n",
    "\n",
    "logger.info('Processing models...')\n",
    "for model in exp.models:\n",
    "    model_result = []\n",
    "    logger.info(f'Getting scores for {model.name}...')\n",
    "\n",
    "    for user_id, user_index in tqdm(exp.eval_method.train_set.uid_map.items()):\n",
    "        try:\n",
    "            scores = model.score(user_index)\n",
    "        except ScoreException:\n",
    "            logger.error(f\"{model.name}: Couldn't predict for user {user_index} ({user_id=})\")\n",
    "            continue\n",
    "\n",
    "        top_items = list(reversed(scores.argsort()))[:k]\n",
    "\n",
    "        for item_id, item_index in exp.eval_method.train_set.iid_map.items():\n",
    "            if item_index not in top_items:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                score = scores[item_index]\n",
    "            except IndexError:\n",
    "                logger.error(\n",
    "                    f\"{model.name}: No score for item {item_index} ({item_id=}) in user {user_index} ({user_id=})\"\n",
    "                )\n",
    "                score = nan\n",
    "\n",
    "            model_result.append({\n",
    "                'user_id': user_id,\n",
    "                'user_index': user_index,\n",
    "                'item_id': item_id,\n",
    "                'item_index': item_index,\n",
    "                'score': score\n",
    "            })\n",
    "\n",
    "    logger.info(f'Exporting {model.name}...')\n",
    "    (DataFrame(model_result)\n",
    "     .sort_values(by=['user_id', 'score'], ascending=[True, False])\n",
    "     .to_csv(results_dir / f'{model.name}.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
