{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69ba81caf58e1381",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Imports\n",
    "These experiments were run on Python 3.8. In the requirements.txt are the versions used for these packages.\n",
    "- tqdm: For showing progress in loops.\n",
    "- numpy and pandas: For data manipulation.\n",
    "- cornac: For obtaining the recommendations.\n",
    "- tensorflow: Required by cornac.\n",
    "- torch: Required for the VAECF implementation of Cornac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from logging import Formatter, StreamHandler, getLogger, INFO\n",
    "\n",
    "from tqdm import tqdm\n",
    "from cornac import Experiment\n",
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.metrics import NDCG, Recall, Precision\n",
    "from cornac.hyperopt import Discrete, Continuous\n",
    "from cornac.hyperopt import GridSearch, RandomSearch\n",
    "#from cornac.models import MF, WMF, SVD, VAECF\n",
    "from cornac.models import VAECF, NeuMF, MCF, SVD, WMF, SKMeans, GMF\n",
    "from cornac.exception import ScoreException\n",
    "from numpy import array, nan\n",
    "from pandas import read_csv, DataFrame, Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419102f5de0b9dd3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Logger setup\n",
    "Here we set up the logger for showing some info when executing this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2daab4dcf47f0e24",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger = getLogger(__name__)\n",
    "logger.setLevel(INFO)\n",
    "\n",
    "ch = StreamHandler()\n",
    "ch.setLevel(INFO)\n",
    "ch.setFormatter(\n",
    "    Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    ")\n",
    "\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c488da52c0b011",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Configuration variables\n",
    "We define some variables used on the rest of the experiment.\n",
    "\n",
    "### General config\n",
    "Getting the date now and the name of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d635b3d3d5b23e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "now = f'{datetime.now():%Y%m%d%H%M}'\n",
    "experiment_name = 'AMBAR'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c6a66141bf440",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### File and dir config\n",
    "Getting the working directory with pathlib, and obtaining the csv to be used in cornac, and defining a results directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b5c46e25bce6b97",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "work_dir = Path('.').resolve()\n",
    "data_file = work_dir / 'data' / experiment_name / 'ratings_info.csv'\n",
    "results_dir = work_dir / 'results' / experiment_name / now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51611283d1977550",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here we make sure the results directory exists by creating it if it doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a635f7c30ba080d2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not results_dir.exists():\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaad9a156a91656b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Also, we make sure the data file exists and is a file. Here we could also make sure that the file is an actual csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d97ea640067e44fd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not data_file.exists() and data_file.is_file():\n",
    "    print(\"Bad data file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5bb98578bc357",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Dataframe config\n",
    "We define the names of the headers of each column to be identified by pandas. Also, we define the data type of the values in each cell of the user, item and rating. If the data has multiple data types, the val_dtype can be a list of type string compatible with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "79e39b6dbc03ed72",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_names = {\n",
    "    'user': 'user_id',\n",
    "    'item': 'track_id',\n",
    "    'rating': 'rating'\n",
    "}\n",
    "val_dtype = 'int'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f3e896f286446",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Cornac config\n",
    "Here we set up the k value, the test set size and the validation set size. Also we decide if we want to exclude unknown values or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f569e7c7b4df6096",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 1000\n",
    "test_size = 0.2\n",
    "val_size = 0.1\n",
    "exclude_unknown = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab5851fd6f35830",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Function setup\n",
    "We set up various utility functions to be used later. Mostly for exporting data and getting it in a format compatible with cornac.\n",
    "\n",
    "set_data_to_tuple_list takes a dict of {user: [item_list, rating_list]}, process it and returns a tuple list of format [(user, item, rating)...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f809e7ca6f8cedc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_data_to_tuple_list(d: dict) -> list:\n",
    "    result = []\n",
    "    for user in d:\n",
    "        transpose = array(d[user]).T\n",
    "        for t in transpose:\n",
    "            result.append((user,) + tuple(t))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5bd6bbdad0dba2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "list_to_dict converts a list into a dict using dict comprehension and enumerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "692ef4f13a3a5c51",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def list_to_dict(l: list) -> dict:\n",
    "    return {i: v for i, v in enumerate(l)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b75cb4025f5f7fa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "get_set_dataframe process the raw data ({user: [item_list, rating_list]}), with the item ids and user ids, and converts it into a pandas DataFrame to be exported later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d0f5aba5f16befe8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transforma de formato ({user:[item_list, rating_list]})\n",
    "# DataFrame final:\n",
    "#    user_id  item_id  rating  item_idx  user_idx\n",
    "# 0        0        0       5         0         0\n",
    "# 1        0        1       3         1         0\n",
    "# 2        1        1       4         1         1\n",
    "# 3        1        2       2         2         1\n",
    "def get_set_dataframe(set_data: dict, i_ids: list, u_ids: list) -> DataFrame:\n",
    "    data_list = set_data_to_tuple_list(set_data)\n",
    "    i_map = list_to_dict(i_ids)\n",
    "    u_map = list_to_dict(u_ids)\n",
    "\n",
    "    set_df = DataFrame(data_list,\n",
    "                       columns=list(col_names.values()),\n",
    "                       dtype=val_dtype)\n",
    "    set_df['item_idx'] = set_df[col_names['item']]\n",
    "    set_df['item'] = set_df[col_names['item']].replace(to_replace=i_map)\n",
    "    set_df['user_idx'] = set_df[col_names['user']]\n",
    "    set_df['user'] = set_df[col_names['user']].replace(to_replace=u_map)\n",
    "    return set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2f3457a3965bf91",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 01:37:38,640 - INFO - Experiment start...\n",
      "2025-01-08 01:37:38,640 - INFO - Experiment start...\n",
      "2025-01-08 01:37:38,640 - INFO - Experiment start...\n",
      "2025-01-08 01:37:38,642 - INFO - AMBAR\n",
      "2025-01-08 01:37:38,642 - INFO - AMBAR\n",
      "2025-01-08 01:37:38,642 - INFO - AMBAR\n",
      "2025-01-08 01:37:38,644 - INFO - k=1000\n",
      "2025-01-08 01:37:38,644 - INFO - k=1000\n",
      "2025-01-08 01:37:38,644 - INFO - k=1000\n",
      "2025-01-08 01:37:38,646 - INFO - work_dir=WindowsPath('M:/Framework/AMBAR')\n",
      "2025-01-08 01:37:38,646 - INFO - work_dir=WindowsPath('M:/Framework/AMBAR')\n",
      "2025-01-08 01:37:38,646 - INFO - work_dir=WindowsPath('M:/Framework/AMBAR')\n",
      "2025-01-08 01:37:38,647 - INFO - data_file=WindowsPath('M:/Framework/AMBAR/data/AMBAR/ratings_info.csv')\n",
      "2025-01-08 01:37:38,647 - INFO - data_file=WindowsPath('M:/Framework/AMBAR/data/AMBAR/ratings_info.csv')\n",
      "2025-01-08 01:37:38,647 - INFO - data_file=WindowsPath('M:/Framework/AMBAR/data/AMBAR/ratings_info.csv')\n",
      "2025-01-08 01:37:38,649 - INFO - results_dir=WindowsPath('M:/Framework/AMBAR/results/AMBAR/202501080137')\n",
      "2025-01-08 01:37:38,649 - INFO - results_dir=WindowsPath('M:/Framework/AMBAR/results/AMBAR/202501080137')\n",
      "2025-01-08 01:37:38,649 - INFO - results_dir=WindowsPath('M:/Framework/AMBAR/results/AMBAR/202501080137')\n"
     ]
    }
   ],
   "source": [
    "logger.info('Experiment start...')\n",
    "logger.info(f'{experiment_name}')\n",
    "logger.info(f'{k=}')\n",
    "logger.info(f'{work_dir=}')\n",
    "logger.info(f'{data_file=}')\n",
    "logger.info(f'{results_dir=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7017668614f3b926",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here we create the dataset out of the data file, the expected data is only with user, item and rating in that order. The name of the columns is defined in the set-up part, same with the data types.\n",
    "\n",
    "For testing purposes before actually executing the full experiment, we left a filter that takes a sample of 50 users, and gets only the data of those 50 users. Please use it only to make sure that the script executes correctly from start to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d16d3a7ebfcee39b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 01:37:38,673 - INFO - Loading data into triplets...\n",
      "2025-01-08 01:37:38,673 - INFO - Loading data into triplets...\n",
      "2025-01-08 01:37:38,673 - INFO - Loading data into triplets...\n"
     ]
    }
   ],
   "source": [
    "# user, item, rating\n",
    "keys = ['0', '1', '2']\n",
    "\n",
    "# Crea un diccionario que mapea cada clave a su tipo de dato\n",
    "if isinstance(val_dtype, str):\n",
    "    d_type = {key: val_dtype for key in keys}\n",
    "elif isinstance(val_dtype, list):\n",
    "    d_type = dict(zip(keys, val_dtype))\n",
    "else:\n",
    "    logger.error('Wrong type setup. Must be a type string or a list of type string.')\n",
    "    exit()\n",
    "\n",
    "logger.info('Loading data into triplets...')\n",
    "df = read_csv(\n",
    "    data_file,\n",
    "    header=0,\n",
    "    names=['0', '1', '2']\n",
    ")[['0', '1', '2']].astype(d_type)\n",
    "\n",
    "# FOR TESTING ONLY\n",
    "# Selecciona aleatoriamente 50 usuarios unicos del dataframe\n",
    "user_filter = Series(df['0'].unique()).sample(50).to_list()\n",
    "# Incluye solo filas donde estan los usuarios filtrados\n",
    "df = df[df['0'].isin(user_filter)]\n",
    "\n",
    "data = list(df.to_records(index=False, column_dtypes=d_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300bbb3260fd4305",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here we create the Ratio Split that will be used by cornac. It splits the data into 3 sets randomly. 1 for test, 1 for train and 1 for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c9e6e383e56136f1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 01:37:38,749 - INFO - Creating ratio split...\n",
      "2025-01-08 01:37:38,749 - INFO - Creating ratio split...\n",
      "2025-01-08 01:37:38,749 - INFO - Creating ratio split...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating_threshold = 1.0\n",
      "exclude_unknowns = True\n",
      "---\n",
      "Training data:\n",
      "Number of users = 50\n",
      "Number of items = 2446\n",
      "Number of ratings = 2959\n",
      "Max rating = 5.0\n",
      "Min rating = 1.0\n",
      "Global mean = 1.6\n",
      "---\n",
      "Test data:\n",
      "Number of users = 50\n",
      "Number of items = 2446\n",
      "Number of ratings = 296\n",
      "Number of unknown users = 0\n",
      "Number of unknown items = 0\n",
      "---\n",
      "Validation data:\n",
      "Number of users = 50\n",
      "Number of items = 2446\n",
      "Number of ratings = 164\n",
      "---\n",
      "Total users = 50\n",
      "Total items = 2446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Juan Jose\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cornac\\data\\dataset.py:335: UserWarning: 141 duplicated observations are removed!\n",
      "  warnings.warn(\"%d duplicated observations are removed!\" % dup_count)\n",
      "c:\\Users\\Juan Jose\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cornac\\data\\dataset.py:335: UserWarning: 6 duplicated observations are removed!\n",
      "  warnings.warn(\"%d duplicated observations are removed!\" % dup_count)\n",
      "c:\\Users\\Juan Jose\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cornac\\data\\dataset.py:335: UserWarning: 1 duplicated observations are removed!\n",
      "  warnings.warn(\"%d duplicated observations are removed!\" % dup_count)\n"
     ]
    }
   ],
   "source": [
    "logger.info('Creating ratio split...')\n",
    "ratio_split = RatioSplit(\n",
    "    data=data,\n",
    "    test_size=test_size,\n",
    "    val_size=val_size,\n",
    "    exclude_unknowns=exclude_unknown,\n",
    "    verbose=True,\n",
    "    seed=123 # Añadida para poder reproducir resultados\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab41cdeeccb9adf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We define the metris here. In this experiment, we set up NDCG, Recall and Precision, using the k defined in the set-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "34a2aa4217a63dfc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    NDCG(k),\n",
    "    Recall(k),\n",
    "    Precision(k)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e4a74ce9da083",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Also, we define the models with some previously obtained parameters. We could also define the hyperparameter calculation in this part, in this case, is important to leave a models variable with said configuration, so cornac can pick up the array and execute the calculation and exporting of the recommendations.\n",
    "\n",
    "Because this script is assuming an array with models with parameters already predefined, in case of needing the best parameters obtained by cornac, the exporting of this must be done after running the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaeb6cc",
   "metadata": {},
   "source": [
    "## Base models to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8bfd5c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_vaecf = VAECF(\n",
    "    name='vaecf_default',\n",
    "    k=k,\n",
    "    autoencoder_structure=[20],\n",
    "    act_fn=\"tanh\",\n",
    "    likelihood=\"mult\",\n",
    "    n_epochs=100,\n",
    "    batch_size=100,\n",
    "    learning_rate=0.001,\n",
    "    beta=1.0,\n",
    "    use_gpu=True,\n",
    "        verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "03840305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos que le pasaremos a CORNAC para ejecutar los experimentos \n",
    "models = [\n",
    "   base_vaecf\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e9d059a8baf22a2d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 01:37:38,840 - INFO - total_users=50\n",
      "2025-01-08 01:37:38,840 - INFO - total_users=50\n",
      "2025-01-08 01:37:38,840 - INFO - total_users=50\n",
      "2025-01-08 01:37:38,842 - INFO - total_items=2446\n",
      "2025-01-08 01:37:38,842 - INFO - total_items=2446\n",
      "2025-01-08 01:37:38,842 - INFO - total_items=2446\n"
     ]
    }
   ],
   "source": [
    "# Obtener el total de usuarios del split de entrenamiento\n",
    "total_users = ratio_split.train_set.num_users\n",
    "# Obtener el total de items del split de entrenamiento\n",
    "total_items = ratio_split.train_set.num_items\n",
    "logger.info(f'{total_users=}')\n",
    "logger.info(f'{total_items=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d75765eba73247",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After setting up the metrics and models, we export the test, train and validation data into the results directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cd3120168e041a6a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 01:37:38,858 - INFO - Exporting training data for MOReGIn...\n",
      "2025-01-08 01:37:38,858 - INFO - Exporting training data for MOReGIn...\n",
      "2025-01-08 01:37:38,858 - INFO - Exporting training data for MOReGIn...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "logger.info('Exporting training data for MOReGIn...')\n",
    "train_df = get_set_dataframe(\n",
    "    dict(ratio_split.train_set.user_data),\n",
    "    list(ratio_split.train_set.item_ids),\n",
    "    list(ratio_split.train_set.user_ids)\n",
    ")\n",
    "\n",
    "# Read the original input data to get continent and genre information\n",
    "input_df = pd.read_csv(work_dir / 'for_testing.csv')\n",
    "input_lookup = input_df.set_index(['user', 'item'])[['continent', 'genre']]\n",
    "\n",
    "# Add continent and genre to training data\n",
    "train_df = train_df.join(input_lookup, on=['user', 'item'])\n",
    "train_df.to_csv('train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3cd0d1fd06dba550",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 01:37:39,121 - INFO - Exporting train data...\n",
      "2025-01-08 01:37:39,121 - INFO - Exporting train data...\n",
      "2025-01-08 01:37:39,121 - INFO - Exporting train data...\n"
     ]
    }
   ],
   "source": [
    "logger.info('Exporting train data...')\n",
    "get_set_dataframe(\n",
    "    dict(ratio_split.train_set.user_data),\n",
    "    list(ratio_split.train_set.item_ids),\n",
    "    list(ratio_split.train_set.user_ids),\n",
    ").to_csv(results_dir / 'train_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f6986140f5a050d1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 01:37:39,292 - INFO - Exporting validation data...\n",
      "2025-01-08 01:37:39,292 - INFO - Exporting validation data...\n",
      "2025-01-08 01:37:39,292 - INFO - Exporting validation data...\n"
     ]
    }
   ],
   "source": [
    "logger.info('Exporting validation data...')\n",
    "get_set_dataframe(\n",
    "    dict(ratio_split.val_set.user_data),\n",
    "    list(ratio_split.val_set.item_ids),\n",
    "    list(ratio_split.val_set.user_ids),\n",
    ").to_csv(results_dir / 'val_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc57886ed53253",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And we run the experiments with the defined variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eee47a93bb6b755c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 01:37:39,400 - INFO - Running experiment...\n",
      "2025-01-08 01:37:39,400 - INFO - Running experiment...\n",
      "2025-01-08 01:37:39,400 - INFO - Running experiment...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[vaecf_default] Training started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 93.99it/s, loss=9.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[vaecf_default] Evaluation started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking: 100%|██████████| 41/41 [00:00<00:00, 911.09it/s]\n",
      "Ranking: 100%|██████████| 34/34 [00:00<00:00, 944.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALIDATION:\n",
      "...\n",
      "              | NDCG@1000 | Precision@1000 | Recall@1000 | Time (s)\n",
      "------------- + --------- + -------------- + ----------- + --------\n",
      "vaecf_default |    0.1478 |         0.0033 |      0.6623 |   0.0390\n",
      "\n",
      "TEST:\n",
      "...\n",
      "              | NDCG@1000 | Precision@1000 | Recall@1000 | Train (s) | Test (s)\n",
      "------------- + --------- + -------------- + ----------- + --------- + --------\n",
      "vaecf_default |    0.1638 |         0.0046 |      0.6297 |    1.0690 |   0.0480\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info('Running experiment...')\n",
    "exp = Experiment(\n",
    "    eval_method=ratio_split,\n",
    "    models=models,\n",
    "    metrics=metrics,\n",
    "    user_based=True,\n",
    ")\n",
    "exp.run()\n",
    "\n",
    "#print(rs_mcf.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faff40fa70f6cf3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After running the experiment, we export the metrics obtained from the calculation into a csv using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4b07b42b349bb38a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 01:37:40,577 - INFO - Exporting metrics...\n",
      "2025-01-08 01:37:40,577 - INFO - Exporting metrics...\n",
      "2025-01-08 01:37:40,577 - INFO - Exporting metrics...\n"
     ]
    }
   ],
   "source": [
    "logger.info('Exporting metrics...')\n",
    "metric_results = {\n",
    "    exp.models[i].name: dict(exp.result[i].metric_avg_results)\n",
    "    for i in range(len(models))\n",
    "}\n",
    "(DataFrame(metric_results)\n",
    " .reset_index()\n",
    " .rename(columns={'index': 'metric'})\n",
    " .to_csv(results_dir / 'metric_results.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4ad9fa00e924ff",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And finally we export the recommendations. We use a custom multi loop to get the results.\n",
    "- Here we first loop over the models of the experiment.\n",
    "- We loop over the users map of cornac to get both the original id and the internal index of cornac.\n",
    "- We get the scores for the users.\n",
    "- We get the k top items using a combination of argsort and reversing of the list.\n",
    "- We loop over the items map of cornac to get both the original id and the internal index of cornac.\n",
    "- We get the score obtained from cornac, or nan in case of IndexError.\n",
    "- We append the user and items, both the id and indexes, and the score to the result list.\n",
    "- After all the loops are finished, we export the data into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fa3b156d0a731c3e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 01:37:40,597 - INFO - Processing models...\n",
      "2025-01-08 01:37:40,597 - INFO - Processing models...\n",
      "2025-01-08 01:37:40,597 - INFO - Processing models...\n",
      "2025-01-08 01:37:40,611 - INFO - Getting scores for vaecf_default...\n",
      "2025-01-08 01:37:40,611 - INFO - Getting scores for vaecf_default...\n",
      "2025-01-08 01:37:40,611 - INFO - Getting scores for vaecf_default...\n",
      "100%|██████████| 50/50 [00:15<00:00,  3.32it/s]\n",
      "2025-01-08 01:38:00,848 - INFO - Exporting vaecf_default...\n",
      "2025-01-08 01:38:00,848 - INFO - Exporting vaecf_default...\n",
      "2025-01-08 01:38:00,848 - INFO - Exporting vaecf_default...\n"
     ]
    }
   ],
   "source": [
    "logger.info('Processing models...')\n",
    "for model in exp.models:\n",
    "    model_result = []\n",
    "    logger.info(f'Getting scores for {model.name}...')\n",
    "\n",
    "    # Read the input data with continent and genre information\n",
    "    input_df = pd.read_csv(work_dir / 'for_testing.csv')\n",
    "    # Convert IDs to integers to ensure matching\n",
    "    input_df['user'] = input_df['user'].astype(int)\n",
    "    input_df['item'] = input_df['item'].astype(int)\n",
    "    \n",
    "    # Create a dictionary for faster lookups - store all genre/continent info for each item\n",
    "    item_info = {}\n",
    "    for _, row in input_df.iterrows():\n",
    "        if row['item'] not in item_info:\n",
    "            item_info[row['item']] = {\n",
    "                'continent': row['continent'],\n",
    "                'genre': row['genre']\n",
    "            }\n",
    "\n",
    "    for user_id, user_index in tqdm(exp.eval_method.train_set.uid_map.items()):\n",
    "        try:\n",
    "            scores = model.score(user_index)\n",
    "        except ScoreException:\n",
    "            logger.error(f\"{model.name}: Couldn't predict for user {user_index} ({user_id=})\")\n",
    "            continue\n",
    "\n",
    "        top_items = list(reversed(scores.argsort()))[:k]\n",
    "\n",
    "        for item_id, item_index in exp.eval_method.train_set.iid_map.items():\n",
    "            if item_index not in top_items:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                score = scores[item_index]\n",
    "            except IndexError:\n",
    "                logger.error(\n",
    "                    f\"{model.name}: No score for item {item_index} ({item_id=}) in user {user_index} ({user_id=})\"\n",
    "                )\n",
    "                score = nan\n",
    "\n",
    "            # Get continent and genre using item-based lookup\n",
    "            item_data = item_info.get(item_id, {})\n",
    "            continent = item_data.get('continent', 'unknown')\n",
    "            genre = item_data.get('genre', 'unknown')\n",
    "\n",
    "            model_result.append({\n",
    "                'id': len(model_result) + 1,\n",
    "                'user': user_id,\n",
    "                'item': item_id,\n",
    "                'rating': score,\n",
    "                'position': list(reversed(scores.argsort())).index(item_index) + 1,\n",
    "                'continent': continent,\n",
    "                'genre': genre\n",
    "            })\n",
    "\n",
    "    logger.info(f'Exporting {model.name}...')\n",
    "    result_df = pd.DataFrame(model_result)\n",
    "    # Save both formats - one for general use and one specifically for MOReGIn\n",
    "    result_df.to_csv(results_dir / f'{model.name}.csv', index=False)\n",
    "    result_df.to_csv(f'{model.name}.csv', index=False)  # This one for MOReGIn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
